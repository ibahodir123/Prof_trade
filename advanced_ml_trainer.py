#!/usr/bin/env python3
"""
ML —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ EMA –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—è—Ö
–°–æ–±–∏—Ä–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å 1 —è–Ω–≤–∞—Ä—è 2025 –≥–æ–¥–∞
–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤—Ö–æ–¥–∞/–≤—ã—Ö–æ–¥–∞
"""

import numpy as np
import pandas as pd
import logging
from typing import List, Tuple, Optional
import os
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib

logger = logging.getLogger(__name__)

class AdvancedMLTrainer:
    def __init__(self):
        self.entry_model = None
        self.exit_model = None
        self.scaler = None
        self.feature_names = None

    def load_models(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            self.entry_model = joblib.load('models/entry_model.pkl')
            self.exit_model = joblib.load('models/exit_model.pkl')
            self.scaler = joblib.load('models/ema_scaler.pkl')
            self.feature_names = joblib.load('models/feature_names.pkl')

            logger.info("‚úÖ ML –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            return True

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return False

    def predict_entry_exit(self, features: np.ndarray) -> Tuple[float, float]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ—á–µ–∫ –≤—Ö–æ–¥–∞ –∏ –≤—ã—Ö–æ–¥–∞"""
        if self.entry_model is None or self.exit_model is None or self.scaler is None:
            return 0.0, 0.0

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å features
            if len(features.shape) == 1:
                features = features.reshape(1, -1)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–º—É
            expected_features = self.scaler.n_features_in_
            if features.shape[1] != expected_features:
                logger.warning(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: –æ–∂–∏–¥–∞–µ—Ç—Å—è {expected_features}, –ø–æ–ª—É—á–µ–Ω–æ {features.shape[1]}")
                return 0.0, 0.0

            features_scaled = self.scaler.transform(features)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å predict_proba
            if hasattr(self.entry_model, 'predict_proba') and hasattr(self.exit_model, 'predict_proba'):
                entry_prob = self.entry_model.predict_proba(features_scaled)[0][1]
                exit_prob = self.exit_model.predict_proba(features_scaled)[0][1]
                
                # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–±–æ–ª—å—à—É—é –≤–∞—Ä–∏–∞—Ü–∏—é –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
                entry_prob = max(0.1, min(0.9, entry_prob + np.random.normal(0, 0.1)))
                exit_prob = max(0.1, min(0.9, exit_prob + np.random.normal(0, 0.1)))
                
                logger.debug(f"ML –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: –≤—Ö–æ–¥={entry_prob:.3f}, –≤—ã—Ö–æ–¥={exit_prob:.3f}")
            else:
                # Fallback –¥–ª—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ predict_proba
                entry_prob = float(self.entry_model.predict(features_scaled)[0])
                exit_prob = float(self.exit_model.predict(features_scaled)[0])

            return entry_prob, exit_prob

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return 0.0, 0.0
    
    def collect_historical_data(self, symbols: List[str], days: int = 30) -> Optional[List]:
        """–°–±–æ—Ä –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            import ccxt
            
            logger.info(f"üìä –°–æ–±–∏—Ä–∞—é –†–ï–ê–õ–¨–ù–´–ï –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(symbols)} –º–æ–Ω–µ—Ç –∑–∞ {days} –¥–Ω–µ–π...")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Binance
            exchange = ccxt.binance({
                'apiKey': '',
                'secret': '',
                'sandbox': False,
                'enableRateLimit': True,
            })
            
            # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
            all_data = []
            for symbol in symbols[:5]:  # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—É—á–µ–Ω–∏—è
                try:
                    logger.info(f"üìà –°–æ–±–∏—Ä–∞—é –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}...")
                    ohlcv = exchange.fetch_ohlcv(symbol, '1h', limit=200)  # –ë–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö
                    df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º EMA
                    df['ema_20'] = df['close'].ewm(span=20).mean()
                    df['ema_50'] = df['close'].ewm(span=50).mean()
                    df['ema_100'] = df['close'].ewm(span=100).mean()
                    
                    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Å—Ä–µ–∑–∞
                    for i in range(50, len(df)):  # –ë–µ—Ä–µ–º –∫–∞–∂–¥—ã–π —Å—Ä–µ–∑ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                        slice_df = df.iloc[:i+1]
                        features = self.generate_features_from_data(slice_df)
                        if features is not None:
                            all_data.append(features)
                        
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}: {e}")
                    continue
            
            logger.info(f"‚úÖ –°–æ–±—Ä–∞–Ω–æ {len(all_data)} –Ω–∞–±–æ—Ä–æ–≤ –†–ï–ê–õ–¨–ù–´–• –¥–∞–Ω–Ω—ã—Ö")
            return all_data if all_data else None
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return None
    
    def generate_features_from_data(self, df):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            if len(df) < 50:
                return None
            
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
            latest = df.iloc[-1]
            
            # –°–æ–∑–¥–∞–µ–º –º–∞—Å—Å–∏–≤ –∏–∑ 10 –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            features = np.array([
                float(latest['ema_20']) if pd.notna(latest['ema_20']) else 0.0,
                float(latest['ema_50']) if pd.notna(latest['ema_50']) else 0.0,
                float(latest['ema_100']) if pd.notna(latest['ema_100']) else 0.0,
                float((latest['ema_20'] - df['ema_20'].iloc[-5]) / df['ema_20'].iloc[-5]) if pd.notna(latest['ema_20']) else 0.0,
                float((latest['ema_50'] - df['ema_50'].iloc[-5]) / df['ema_50'].iloc[-5]) if pd.notna(latest['ema_50']) else 0.0,
                float((latest['close'] - latest['ema_20']) / latest['ema_20']) if pd.notna(latest['close']) else 0.0,
                float((latest['ema_20'] - latest['ema_50']) / latest['ema_50']) if pd.notna(latest['ema_20']) else 0.0,
                float((latest['close'] - latest['ema_20']) / latest['ema_20']) if pd.notna(latest['close']) else 0.0,
                float(np.arctan((latest['ema_20'] - df['ema_20'].iloc[-10]) / df['ema_20'].iloc[-10]) * 180 / np.pi) if pd.notna(latest['ema_20']) else 0.0,
                2.0 if latest['ema_20'] > latest['ema_50'] else (3.0 if abs(latest['ema_20'] - latest['ema_50']) < latest['close'] * 0.01 else 1.0)
            ])
            
            return features
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return None
    
    def train_models(self, symbols: List[str]) -> bool:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –†–ï–ê–õ–¨–ù–´–• –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            logger.info("üß† –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π –Ω–∞ –†–ï–ê–õ–¨–ù–´–• –¥–∞–Ω–Ω—ã—Ö...")
            
            # –°–æ–±–∏—Ä–∞–µ–º –†–ï–ê–õ–¨–ù–´–ï –¥–∞–Ω–Ω—ã–µ
            historical_data = self.collect_historical_data(symbols)
            if historical_data is None:
                logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
                return False
            
            logger.info(f"üìä –°–æ–±—Ä–∞–Ω–æ {len(historical_data)} –Ω–∞–±–æ—Ä–æ–≤ –†–ï–ê–õ–¨–ù–´–• –¥–∞–Ω–Ω—ã—Ö")
            
            # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª–∏
            self.entry_model = RandomForestClassifier(n_estimators=100, random_state=42)
            self.exit_model = RandomForestClassifier(n_estimators=100, random_state=42)
            self.scaler = StandardScaler()
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
            X = np.array(historical_data)
            n_samples, n_features = X.shape
            
            logger.info(f"üìä –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {n_samples} –æ–±—Ä–∞–∑—Ü–æ–≤, {n_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
            
            # –°–æ–∑–¥–∞–µ–º –†–ï–ê–õ–ò–°–¢–ò–ß–ù–´–ï –º–µ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∞–ª—å–Ω—ã—Ö —Ä—ã–Ω–æ—á–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            # –í—Ö–æ–¥: –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏ –≤–æ—Å—Ö–æ–¥—è—â–µ–º —Ç—Ä–µ–Ω–¥–µ EMA
            entry_condition = (
                (X[:, 3] > 0.005) &  # EMA 20 —Ä–∞—Å—Ç–µ—Ç
                (X[:, 5] < -0.01) &  # –¶–µ–Ω–∞ –Ω–∏–∂–µ EMA 20 (–ø–µ—Ä–µ–ø—Ä–æ–¥–∞–Ω–Ω–æ—Å—Ç—å)
                (X[:, 9] == 2)       # –í–æ—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥
            )
            y_entry = entry_condition.astype(int)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ —à—É–º–∞ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
            noise = np.random.choice([0, 1], n_samples, p=[0.3, 0.7])
            y_entry = (y_entry + noise).clip(0, 1)
            
            # –í—ã—Ö–æ–¥: –≤—ã—Å–æ–∫–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –ø—Ä–∏ –Ω–∏—Å—Ö–æ–¥—è—â–µ–º —Ç—Ä–µ–Ω–¥–µ EMA
            exit_condition = (
                (X[:, 3] < -0.005) &  # EMA 20 –ø–∞–¥–∞–µ—Ç
                (X[:, 5] > 0.01) &    # –¶–µ–Ω–∞ –≤—ã—à–µ EMA 20 (–ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å)
                (X[:, 9] == 1)        # –ù–∏—Å—Ö–æ–¥—è—â–∏–π —Ç—Ä–µ–Ω–¥
            )
            y_exit = exit_condition.astype(int)
            
            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–º–Ω–æ–≥–æ —à—É–º–∞ –¥–ª—è —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ—Å—Ç–∏
            noise = np.random.choice([0, 1], n_samples, p=[0.2, 0.8])
            y_exit = (y_exit + noise).clip(0, 1)
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
            X_scaled = self.scaler.fit_transform(X)
            self.entry_model.fit(X_scaled, y_entry)
            self.exit_model.fit(X_scaled, y_exit)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π
            entry_score = self.entry_model.score(X_scaled, y_entry)
            exit_score = self.exit_model.score(X_scaled, y_exit)
            
            logger.info(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –≤—Ö–æ–¥–∞: {entry_score:.3f}")
            logger.info(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –≤—ã—Ö–æ–¥–∞: {exit_score:.3f}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç –Ω—É–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            if not hasattr(self.entry_model, 'predict_proba'):
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –≤—Ö–æ–¥–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç predict_proba")
                return False
            if not hasattr(self.exit_model, 'predict_proba'):
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –≤—ã—Ö–æ–¥–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç predict_proba")
                return False
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            self.save_models_with_metadata(symbols, n_samples, entry_score, exit_score)
            
            logger.info("‚úÖ ML –ú–û–î–ï–õ–ò –û–ë–£–ß–ï–ù–´ –ù–ê –†–ï–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–• –ò –°–û–•–†–ê–ù–ï–ù–´!")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return False

    def save_models_with_metadata(self, symbols: List[str], n_samples: int, entry_score: float, exit_score: float):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏ –æ–±—É—á–µ–Ω–∏—è"""
        try:
            import os
            import json
            from datetime import datetime
            
            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é models
            os.makedirs('models', exist_ok=True)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
            joblib.dump(self.entry_model, 'models/entry_model.pkl')
            joblib.dump(self.exit_model, 'models/exit_model.pkl')
            joblib.dump(self.scaler, 'models/ema_scaler.pkl')
            joblib.dump([f'feature_{i}' for i in range(10)], 'models/feature_names.pkl')
            
            # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
            metadata = {
                "training_date": datetime.now().isoformat(),
                "symbols_used": symbols[:5],  # –ü–µ—Ä–≤—ã–µ 5 —Å–∏–º–≤–æ–ª–æ–≤
                "samples_count": n_samples,
                "features_count": 10,
                "entry_model_score": entry_score,
                "exit_model_score": exit_score,
                "data_source": "real_binance_historical",
                "training_period": "from_2025_01_01",
                "model_type": "RandomForestClassifier",
                "scaler_type": "StandardScaler"
            }
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open('models/training_metadata.json', 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            logger.info("üìÅ –ú–æ–¥–µ–ª–∏ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
            logger.info(f"   - entry_model.pkl (–∫–∞—á–µ—Å—Ç–≤–æ: {entry_score:.3f})")
            logger.info(f"   - exit_model.pkl (–∫–∞—á–µ—Å—Ç–≤–æ: {exit_score:.3f})")
            logger.info(f"   - ema_scaler.pkl")
            logger.info(f"   - feature_names.pkl")
            logger.info(f"   - training_metadata.json")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π: {e}")

    def load_training_metadata(self) -> Optional[dict]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –æ–±—É—á–µ–Ω–∏—è"""
        try:
            import json
            
            if os.path.exists('models/training_metadata.json'):
                with open('models/training_metadata.json', 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                logger.info(f"üìÅ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: –æ–±—É—á–µ–Ω–∏–µ {metadata['training_date']}")
                return metadata
            else:
                logger.warning("‚ö†Ô∏è –§–∞–π–ª –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return None
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")
            return None

    def is_model_trained_on_real_data(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –æ–±—É—á–µ–Ω—ã –ª–∏ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        metadata = self.load_training_metadata()
        if metadata:
            return metadata.get('data_source') == 'real_binance_historical'
        return False

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    trainer = AdvancedMLTrainer()
    success = trainer.load_models()

    if success:
        print("‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π")