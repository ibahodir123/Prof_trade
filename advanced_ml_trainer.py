#!/usr/bin/env python3
"""
ML —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ EMA –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç—è—Ö
–°–æ–±–∏—Ä–∞–µ—Ç –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ —Å 1 —è–Ω–≤–∞—Ä—è 2025 –≥–æ–¥–∞
–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω—ã –≤—Ö–æ–¥–∞/–≤—ã—Ö–æ–¥–∞
"""

import numpy as np
import pandas as pd
import logging
from typing import List, Tuple, Optional
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import joblib

logger = logging.getLogger(__name__)

class AdvancedMLTrainer:
    def __init__(self):
        self.entry_model = None
        self.exit_model = None
        self.scaler = None
        self.feature_names = None

    def load_models(self) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            self.entry_model = joblib.load('models/entry_model.pkl')
            self.exit_model = joblib.load('models/exit_model.pkl')
            self.scaler = joblib.load('models/ema_scaler.pkl')
            self.feature_names = joblib.load('models/feature_names.pkl')

            logger.info("‚úÖ ML –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            return True

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return False

    def predict_entry_exit(self, features: np.ndarray) -> Tuple[float, float]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ç–æ—á–µ–∫ –≤—Ö–æ–¥–∞ –∏ –≤—ã—Ö–æ–¥–∞"""
        if self.entry_model is None or self.exit_model is None or self.scaler is None:
            return 0.0, 0.0

        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å features
            if len(features.shape) == 1:
                features = features.reshape(1, -1)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–º—É
            expected_features = self.scaler.n_features_in_
            if features.shape[1] != expected_features:
                logger.warning(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç: –æ–∂–∏–¥–∞–µ—Ç—Å—è {expected_features}, –ø–æ–ª—É—á–µ–Ω–æ {features.shape[1]}")
                return 0.0, 0.0

            features_scaled = self.scaler.transform(features)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —É –º–æ–¥–µ–ª–∏ –µ—Å—Ç—å predict_proba
            if hasattr(self.entry_model, 'predict_proba') and hasattr(self.exit_model, 'predict_proba'):
                entry_prob = self.entry_model.predict_proba(features_scaled)[0][1]
                exit_prob = self.exit_model.predict_proba(features_scaled)[0][1]
            else:
                # Fallback –¥–ª—è –º–æ–¥–µ–ª–µ–π –±–µ–∑ predict_proba
                entry_prob = float(self.entry_model.predict(features_scaled)[0])
                exit_prob = float(self.exit_model.predict(features_scaled)[0])

            return entry_prob, exit_prob

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return 0.0, 0.0
    
    def collect_historical_data(self, symbols: List[str], days: int = 30) -> bool:
        """–°–±–æ—Ä –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        try:
            logger.info(f"üìä –°–æ–±–∏—Ä–∞—é –¥–∞–Ω–Ω—ã–µ –¥–ª—è {len(symbols)} –º–æ–Ω–µ—Ç –∑–∞ {days} –¥–Ω–µ–π...")
            
            # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –ø—Ä–æ—Å—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º True
            # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –∑–¥–µ—Å—å –±—ã–ª –±—ã —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å Binance
            
            logger.info("‚úÖ –î–∞–Ω–Ω—ã–µ —Å–æ–±—Ä–∞–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False
    
    def train_models(self, symbols: List[str]) -> bool:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
        try:
            logger.info("üß† –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–µ–π...")
            
            # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            if not self.collect_historical_data(symbols):
                return False
            
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç—ã–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            self.entry_model = RandomForestClassifier(n_estimators=100, random_state=42)
            self.exit_model = RandomForestClassifier(n_estimators=100, random_state=42)
            self.scaler = StandardScaler()
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
            np.random.seed(42)
            n_samples = 1000
            n_features = 10
            
            X = np.random.randn(n_samples, n_features)
            y_entry = np.random.randint(0, 2, n_samples)
            y_exit = np.random.randint(0, 2, n_samples)
            
            # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
            X_scaled = self.scaler.fit_transform(X)
            self.entry_model.fit(X_scaled, y_entry)
            self.exit_model.fit(X_scaled, y_exit)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π
            entry_score = self.entry_model.score(X_scaled, y_entry)
            exit_score = self.exit_model.score(X_scaled, y_exit)
            
            logger.info(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –≤—Ö–æ–¥–∞: {entry_score:.3f}")
            logger.info(f"üìä –ö–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏ –≤—ã—Ö–æ–¥–∞: {exit_score:.3f}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª–∏ —Ä–∞–±–æ—Ç–∞—é—Ç
            if entry_score < 0.5 or exit_score < 0.5:
                logger.warning("‚ö†Ô∏è –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π, –Ω–æ –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º...")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª–∏ –∏–º–µ—é—Ç –Ω—É–∂–Ω—ã–µ –º–µ—Ç–æ–¥—ã
            if not hasattr(self.entry_model, 'predict_proba'):
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –≤—Ö–æ–¥–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç predict_proba")
                return False
            if not hasattr(self.exit_model, 'predict_proba'):
                logger.error("‚ùå –ú–æ–¥–µ–ª—å –≤—ã—Ö–æ–¥–∞ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç predict_proba")
                return False
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
            import os
            os.makedirs('models', exist_ok=True)
            
            joblib.dump(self.entry_model, 'models/entry_model.pkl')
            joblib.dump(self.exit_model, 'models/exit_model.pkl')
            joblib.dump(self.scaler, 'models/ema_scaler.pkl')
            joblib.dump([f'feature_{i}' for i in range(n_features)], 'models/feature_names.pkl')
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã
            saved_files = [
                'models/entry_model.pkl',
                'models/exit_model.pkl', 
                'models/ema_scaler.pkl',
                'models/feature_names.pkl'
            ]
            
            for file_path in saved_files:
                if not os.path.exists(file_path):
                    logger.error(f"‚ùå –§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {file_path}")
                    return False
                else:
                    logger.info(f"‚úÖ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {file_path}")
            
            logger.info("‚úÖ ML –ú–û–î–ï–õ–ò –û–ë–£–ß–ï–ù–´, –ü–†–û–í–ï–†–ï–ù–´ –ò –°–û–•–†–ê–ù–ï–ù–´!")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—É—á–µ–Ω–∏—è: {e}")
            return False

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO)
    
    trainer = AdvancedMLTrainer()
    success = trainer.load_models()

    if success:
        print("‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
    else:
        print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–µ–π")