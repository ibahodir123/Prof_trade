"""
–°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å 1 —è–Ω–≤–∞—Ä—è 2025
–°–æ–±–∏—Ä–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –∏ –æ–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ —Ç—Ä–µ–Ω–¥–∞
"""

import pandas as pd
import numpy as np
import ccxt
import joblib
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from advanced_trend_analyzer import TrendAnalyzer
import logging
from datetime import datetime, timedelta
import os
import json

logger = logging.getLogger(__name__)

class HistoricalTrendTrainer:
    """–¢—Ä–µ–Ω–µ—Ä –º–æ–¥–µ–ª–µ–π –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.exchange = ccxt.binance({
            'apiKey': '',
            'secret': '',
            'sandbox': False,
            'rateLimit': 1200,
        })
        
        self.analyzer = TrendAnalyzer()
        self.start_date = datetime(2025, 1, 1)
        self.models = {}
        
    def collect_historical_data(self, symbols: list, days: int = 30) -> dict:
        """–°–±–æ—Ä –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        logger.info(f"üìä –ù–∞—á–∏–Ω–∞—é —Å–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö —Å {self.start_date} –¥–ª—è {len(symbols)} —Å–∏–º–≤–æ–ª–æ–≤...")
        
        all_data = {}
        
        for symbol in symbols:
            try:
                logger.info(f"üìà –°–æ–±–∏—Ä–∞—é –¥–∞–Ω–Ω—ã–µ –¥–ª—è {symbol}...")
                
                # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å 1 —è–Ω–≤–∞—Ä—è 2025
                since = int(self.start_date.timestamp() * 1000)
                limit = days * 24  # –ß–∞—Å–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
                
                ohlcv = self.exchange.fetch_ohlcv(symbol, '1h', since=since, limit=limit)
                
                if ohlcv:
                    df = pd.DataFrame(ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
                    df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
                    df.set_index('timestamp', inplace=True)
                    
                    all_data[symbol] = df
                    logger.info(f"‚úÖ {symbol}: {len(df)} –∑–∞–ø–∏—Å–µ–π")
                else:
                    logger.warning(f"‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")
                    
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö {symbol}: {e}")
                continue
        
        logger.info(f"üéØ –°–æ–±—Ä–∞–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {len(all_data)} —Å–∏–º–≤–æ–ª–æ–≤")
        return all_data
    
    def prepare_training_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        
        features = pd.DataFrame(index=df.index)
        
        # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        features['rsi'] = self._calculate_rsi(df['close'])
        features['ema_20'] = df['close'].ewm(span=20).mean()
        features['ema_50'] = df['close'].ewm(span=50).mean()
        features['ema_100'] = df['close'].ewm(span=100).mean()
        
        # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
        features['volatility'] = df['close'].rolling(20).std() / df['close'].rolling(20).mean()
        
        # –û–±—ä–µ–º
        features['volume_ma'] = df['volume'].rolling(20).mean()
        features['volume_ratio'] = df['volume'] / features['volume_ma']
        
        # –¶–µ–Ω–æ–≤—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        features['price_change_1h'] = df['close'].pct_change(1)
        features['price_change_4h'] = df['close'].pct_change(4)
        features['price_change_24h'] = df['close'].pct_change(24)
        
        # –ú–∞–∫—Å–∏–º—É–º—ã –∏ –º–∏–Ω–∏–º—É–º—ã
        features['high_20'] = df['high'].rolling(20).max()
        features['low_20'] = df['low'].rolling(20).min()
        
        # –û—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–∏
        features['price_position'] = (df['close'] - features['low_20']) / (features['high_20'] - features['low_20'])
        
        # –¢—Ä–µ–Ω–¥–æ–≤—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
        features['trend_strength'] = (features['ema_20'] - features['ema_50']) / features['ema_50']
        
        # –£–¥–∞–ª—è–µ–º NaN
        features = features.dropna()
        
        return features
    
    def create_training_labels(self, df: pd.DataFrame) -> pd.DataFrame:
        """–°–æ–∑–¥–∞–Ω–∏–µ –º–µ—Ç–æ–∫ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤"""
        
        labels = pd.DataFrame(index=df.index)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—É—é —Ç–æ—á–∫—É –¥–∞–Ω–Ω—ã—Ö
        for i in range(100, len(df)):  # –ù–∞—á–∏–Ω–∞–µ–º —Å 100-–π —Å–≤–µ—á–∏ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–æ–≤
            try:
                # –ë–µ—Ä–µ–º –¥–∞–Ω–Ω—ã–µ –¥–æ —Ç–µ–∫—É—â–µ–π —Ç–æ—á–∫–∏
                historical_data = df.iloc[:i+1]
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ç—Ä–µ–Ω–¥
                analysis = self.analyzer.analyze_trend(historical_data)
                
                if 'error' not in analysis:
                    # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞
                    labels.loc[df.index[i], 'trend'] = analysis['trend_direction']
                    labels.loc[df.index[i], 'phase'] = analysis['current_phase']
                    labels.loc[df.index[i], 'signal'] = analysis['signal']['type']
                    
                    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏
                    if analysis['signal']['type'] in ['LONG', 'SHORT']:
                        labels.loc[df.index[i], 'is_entry_point'] = 1
                        labels.loc[df.index[i], 'confidence'] = analysis['signal'].get('confidence', 0.5)
                    else:
                        labels.loc[df.index[i], 'is_entry_point'] = 0
                        labels.loc[df.index[i], 'confidence'] = 0
                        
            except Exception as e:
                logger.debug(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –∏–Ω–¥–µ–∫—Å–∞ {i}: {e}")
                continue
        
        return labels.dropna()
    
    def train_trend_models(self, historical_data: dict) -> dict:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ç—Ä–µ–Ω–¥–æ–≤"""
        
        logger.info("ü§ñ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        
        all_features = []
        all_labels = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ –≤—Å–µ—Ö —Å–∏–º–≤–æ–ª–æ–≤
        for symbol, df in historical_data.items():
            try:
                logger.info(f"üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {symbol}...")
                
                # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
                features = self.prepare_training_features(df)
                
                # –°–æ–∑–¥–∞–µ–º –º–µ—Ç–∫–∏
                labels = self.create_training_labels(df)
                
                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã
                common_index = features.index.intersection(labels.index)
                features = features.loc[common_index]
                labels = labels.loc[common_index]
                
                if len(features) > 50:  # –ú–∏–Ω–∏–º—É–º –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
                    all_features.append(features)
                    all_labels.append(labels)
                    logger.info(f"‚úÖ {symbol}: {len(features)} –æ–±—Ä–∞–∑—Ü–æ–≤")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {symbol}: {e}")
                continue
        
        if not all_features:
            logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
            return {}
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        combined_features = pd.concat(all_features, ignore_index=True)
        combined_labels = pd.concat(all_labels, ignore_index=True)
        
        logger.info(f"üìä –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(combined_features)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∑–∞–¥–∞—á
        models = {}
        
        # 1. –ú–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞
        if 'trend' in combined_labels.columns:
            X = combined_features
            y = combined_labels['trend']
            
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            trend_model = RandomForestClassifier(n_estimators=100, random_state=42)
            trend_model.fit(X_train, y_train)
            
            models['trend_detector'] = trend_model
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            y_pred = trend_model.predict(X_test)
            logger.info(f"üìä –ú–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç—Ä–µ–Ω–¥–∞:")
            logger.info(classification_report(y_test, y_pred))
        
        # 2. –ú–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–∑—ã
        if 'phase' in combined_labels.columns:
            X = combined_features
            y = combined_labels['phase']
            
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            phase_model = RandomForestClassifier(n_estimators=100, random_state=42)
            phase_model.fit(X_train, y_train)
            
            models['phase_detector'] = phase_model
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            y_pred = phase_model.predict(X_test)
            logger.info(f"üìä –ú–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ñ–∞–∑—ã:")
            logger.info(classification_report(y_test, y_pred))
        
        # 3. –ú–æ–¥–µ–ª—å —Ç–æ—á–µ–∫ –≤—Ö–æ–¥–∞
        if 'is_entry_point' in combined_labels.columns:
            X = combined_features
            y = combined_labels['is_entry_point']
            
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            
            entry_model = RandomForestClassifier(n_estimators=100, random_state=42)
            entry_model.fit(X_train, y_train)
            
            models['entry_detector'] = entry_model
            
            # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
            y_pred = entry_model.predict(X_test)
            logger.info(f"üìä –ú–æ–¥–µ–ª—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ—á–µ–∫ –≤—Ö–æ–¥–∞:")
            logger.info(classification_report(y_test, y_pred))
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        self._save_models(models)
        
        return models
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """–†–∞—Å—á–µ—Ç RSI"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def _save_models(self, models: dict):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        
        os.makedirs('models', exist_ok=True)
        
        for name, model in models.items():
            filename = f'models/{name}.pkl'
            joblib.dump(model, filename)
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –º–æ–¥–µ–ª—å: {filename}")
    
    def load_models(self) -> dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        
        models = {}
        model_files = {
            'trend_detector': 'models/trend_detector.pkl',
            'phase_detector': 'models/phase_detector.pkl', 
            'entry_detector': 'models/entry_detector.pkl'
        }
        
        for name, filename in model_files.items():
            try:
                if os.path.exists(filename):
                    models[name] = joblib.load(filename)
                    logger.info(f"üìÇ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {name}")
                else:
                    logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {filename}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {name}: {e}")
        
        return models
    
    def predict_with_models(self, features: pd.DataFrame, models: dict) -> dict:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        
        predictions = {}
        
        if 'trend_detector' in models:
            predictions['trend'] = models['trend_detector'].predict(features.iloc[-1:].values)[0]
            predictions['trend_proba'] = models['trend_detector'].predict_proba(features.iloc[-1:].values)[0]
        
        if 'phase_detector' in models:
            predictions['phase'] = models['phase_detector'].predict(features.iloc[-1:].values)[0]
            predictions['phase_proba'] = models['phase_detector'].predict_proba(features.iloc[-1:].values)[0]
        
        if 'entry_detector' in models:
            predictions['entry_probability'] = models['entry_detector'].predict_proba(features.iloc[-1:].values)[0][1]
        
        return predictions

def train_models_on_historical_data():
    """–§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö"""
    
    # –°–ø–∏—Å–æ–∫ —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    symbols = [
        'BTC/USDT', 'ETH/USDT', 'BNB/USDT', 'ADA/USDT', 'SOL/USDT',
        'XRP/USDT', 'DOT/USDT', 'DOGE/USDT', 'AVAX/USDT', 'MATIC/USDT'
    ]
    
    trainer = HistoricalTrendTrainer()
    
    # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    historical_data = trainer.collect_historical_data(symbols, days=30)
    
    if not historical_data:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
        return
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
    models = trainer.train_trend_models(historical_data)
    
    if models:
        logger.info(f"üéØ –û–±—É—á–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π!")
        return models
    else:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª–∏")
        return None

if __name__ == "__main__":
    train_models_on_historical_data()




