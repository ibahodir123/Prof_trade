#!/usr/bin/env python3
"""
–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å—Ç—Ä–µ–ª—è—é—â–∏—Ö –º–æ–Ω–µ—Ç
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç LSTM + Dense —Å–ª–æ–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
"""
import pandas as pd
import numpy as np
import json
import pickle
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional
import logging

# ML –±–∏–±–ª–∏–æ—Ç–µ–∫–∏
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import to_categorical

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ShootingStarPredictor:
    """–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å—Ç—Ä–µ–ª—è—é—â–∏—Ö –º–æ–Ω–µ—Ç"""
    
    def __init__(self, sequence_length: int = 24, prediction_horizon: int = 24):
        """
        Args:
            sequence_length: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —á–∞—Å–æ–≤ –∏—Å—Ç–æ—Ä–∏–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (24 —á–∞—Å–∞)
            prediction_horizon: –ù–∞ —Å–∫–æ–ª—å–∫–æ —á–∞—Å–æ–≤ –≤–ø–µ—Ä–µ–¥ –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º (24 —á–∞—Å–∞)
        """
        self.sequence_length = sequence_length
        self.prediction_horizon = prediction_horizon
        self.scaler = StandardScaler()
        self.label_encoder = LabelEncoder()
        self.model = None
        self.feature_columns = []
        self.is_trained = False
        
    def prepare_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        if df.empty:
            return df
            
        try:
            # –ë–∞–∑–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = pd.DataFrame(index=df.index)
            
            # –¶–µ–Ω–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features['price'] = df['close']
            features['price_change_1h'] = df['close'].pct_change(1)
            features['price_change_4h'] = df['close'].pct_change(4)
            features['price_change_12h'] = df['close'].pct_change(12)
            features['price_change_24h'] = df['close'].pct_change(24)
            
            # EMA –æ—Ç–Ω–æ—à–µ–Ω–∏—è
            features['price_to_ema20'] = df['close'] / df['ema_20']
            features['price_to_ema50'] = df['close'] / df['ema_50']
            features['price_to_ema100'] = df['close'] / df['ema_100']
            features['ema20_to_ema50'] = df['ema_20'] / df['ema_50']
            features['ema50_to_ema100'] = df['ema_50'] / df['ema_100']
            
            # RSI –∏ –µ–≥–æ –∏–∑–º–µ–Ω–µ–Ω–∏—è
            features['rsi'] = df['rsi']
            features['rsi_change'] = df['rsi'].diff()
            
            # Bollinger Bands
            features['bb_position'] = (df['close'] - df['bb_lower']) / (df['bb_upper'] - df['bb_lower'])
            features['bb_width'] = (df['bb_upper'] - df['bb_lower']) / df['bb_middle']
            
            # MACD
            features['macd'] = df['macd']
            features['macd_signal'] = df['macd_signal']
            features['macd_histogram'] = df['macd_histogram']
            
            # –û–±—ä–µ–º–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features['volume'] = df['volume']
            features['volume_change'] = df['volume'].pct_change()
            features['volume_ratio'] = df['volume_ratio']
            
            # –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
            features['volatility'] = df['volatility']
            features['high_low_ratio'] = df['high'] / df['low']
            features['open_close_ratio'] = df['open'] / df['close']
            
            # –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
            features['hammer'] = ((df['close'] - df['low']) / (df['high'] - df['low']) > 0.6).astype(int)
            features['doji'] = (abs(df['open'] - df['close']) / (df['high'] - df['low']) < 0.1).astype(int)
            features['green_candle'] = (df['close'] > df['open']).astype(int)
            
            # Momentum –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
            features['price_momentum'] = df['close'].rolling(12).apply(lambda x: (x[-1] - x[0]) / x[0])
            features['volume_momentum'] = df['volume'].rolling(12).apply(lambda x: (x[-1] - x[0]) / x[0])
            
            # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
            features['hour'] = df.index.hour
            features['day_of_week'] = df.index.dayofweek
            features['is_weekend'] = (df.index.dayofweek >= 5).astype(int)
            
            return features.dropna()
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            return pd.DataFrame()
    
    def create_sequences(self, features: pd.DataFrame, targets: pd.Series) -> Tuple[np.ndarray, np.ndarray]:
        """–°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è LSTM"""
        try:
            X, y = [], []
            
            for i in range(self.sequence_length, len(features) - self.prediction_horizon + 1):
                # –í—Ö–æ–¥–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
                X.append(features.iloc[i-self.sequence_length:i].values)
                
                # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è
                y.append(targets.iloc[i + self.prediction_horizon - 1])
            
            return np.array(X), np.array(y)
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π: {e}")
            return np.array([]), np.array([])
    
    def build_model(self, input_shape: Tuple[int, int], num_classes: int) -> Sequential:
        """–°—Ç—Ä–æ–∏—Ç –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏"""
        model = Sequential([
            # LSTM —Å–ª–æ–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤
            LSTM(128, return_sequences=True, input_shape=input_shape),
            BatchNormalization(),
            Dropout(0.2),
            
            LSTM(64, return_sequences=True),
            BatchNormalization(),
            Dropout(0.2),
            
            LSTM(32, return_sequences=False),
            BatchNormalization(),
            Dropout(0.2),
            
            # Dense —Å–ª–æ–∏ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.3),
            
            Dense(32, activation='relu'),
            Dropout(0.2),
            
            Dense(num_classes, activation='softmax')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='categorical_crossentropy',
            metrics=['accuracy', 'precision', 'recall']
        )
        
        return model
    
    def train(self, data: Dict[str, pd.DataFrame], test_size: float = 0.2):
        """–û–±—É—á–∞–µ—Ç –Ω–µ–π—Ä–æ–Ω–Ω—É—é —Å–µ—Ç—å"""
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏...")
        
        all_features = []
        all_targets = []
        
        # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ —Å–æ –≤—Å–µ—Ö –º–æ–Ω–µ—Ç
        for symbol, df in data.items():
            if df.empty or 'is_shooting_star' not in df.columns:
                continue
                
            logger.info(f"üìä –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é {symbol}...")
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self.prepare_features(df)
            if features.empty:
                continue
            
            # –¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è - –∫–∞—Ç–µ–≥–æ—Ä–∏—è —Ä–æ—Å—Ç–∞
            targets = df['growth_category'].loc[features.index]
            
            # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            X_seq, y_seq = self.create_sequences(features, targets)
            
            if len(X_seq) > 0:
                all_features.append(X_seq)
                all_targets.append(y_seq)
        
        if not all_features:
            logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            return False
        
        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ
        X = np.concatenate(all_features, axis=0)
        y = np.concatenate(all_targets, axis=0)
        
        logger.info(f"üìä –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {X.shape}")
        logger.info(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤: {np.bincount(y.astype(int))}")
        
        # –ö–æ–¥–∏—Ä—É–µ–º —Ü–µ–ª–µ–≤—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
        y_encoded = to_categorical(y, num_classes=5)
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y_encoded, test_size=test_size, random_state=42, stratify=y
        )
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        X_train_reshaped = X_train.reshape(-1, X_train.shape[-1])
        X_test_reshaped = X_test.reshape(-1, X_test.shape[-1])
        
        X_train_scaled = self.scaler.fit_transform(X_train_reshaped)
        X_test_scaled = self.scaler.transform(X_test_reshaped)
        
        X_train_scaled = X_train_scaled.reshape(X_train.shape)
        X_test_scaled = X_test_scaled.reshape(X_test.shape)
        
        # –°—Ç—Ä–æ–∏–º –º–æ–¥–µ–ª—å
        self.model = self.build_model((X_train.shape[1], X_train.shape[2]), 5)
        
        # Callbacks
        callbacks = [
            EarlyStopping(patience=10, restore_best_weights=True),
            ReduceLROnPlateau(factor=0.5, patience=5)
        ]
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        history = self.model.fit(
            X_train_scaled, y_train,
            validation_data=(X_test_scaled, y_test),
            epochs=100,
            batch_size=32,
            callbacks=callbacks,
            verbose=1
        )
        
        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        y_pred = self.model.predict(X_test_scaled)
        y_pred_classes = np.argmax(y_pred, axis=1)
        y_test_classes = np.argmax(y_test, axis=1)
        
        accuracy = accuracy_score(y_test_classes, y_pred_classes)
        logger.info(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏: {accuracy:.4f}")
        
        # –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
        class_names = ['–ë–µ–∑ —Ä–æ—Å—Ç–∞', '–ú–∞–ª—ã–π —Ä–æ—Å—Ç', '–°—Ä–µ–¥–Ω–∏–π —Ä–æ—Å—Ç', '–ë–æ–ª—å—à–æ–π —Ä–æ—Å—Ç', '–û–≥—Ä–æ–º–Ω—ã–π —Ä–æ—Å—Ç']
        report = classification_report(y_test_classes, y_pred_classes, target_names=class_names)
        logger.info(f"üìä –û—Ç—á–µ—Ç –ø–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n{report}")
        
        self.is_trained = True
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.save_model()
        
        return True
    
    def predict(self, df: pd.DataFrame) -> Dict:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å—Ç—Ä–µ–ª—è—é—â–∏—Ö –º–æ–Ω–µ—Ç"""
        if not self.is_trained or self.model is None:
            logger.error("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞")
            return {}
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = self.prepare_features(df)
            if features.empty or len(features) < self.sequence_length:
                return {}
            
            # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å
            last_sequence = features.iloc[-self.sequence_length:].values
            last_sequence = last_sequence.reshape(1, self.sequence_length, -1)
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º
            last_sequence_reshaped = last_sequence.reshape(-1, last_sequence.shape[-1])
            last_sequence_scaled = self.scaler.transform(last_sequence_reshaped)
            last_sequence_scaled = last_sequence_scaled.reshape(last_sequence.shape)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            predictions = self.model.predict(last_sequence_scaled)
            probabilities = predictions[0]
            
            # –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            class_names = ['–ë–µ–∑ —Ä–æ—Å—Ç–∞', '–ú–∞–ª—ã–π —Ä–æ—Å—Ç', '–°—Ä–µ–¥–Ω–∏–π —Ä–æ—Å—Ç', '–ë–æ–ª—å—à–æ–π —Ä–æ—Å—Ç', '–û–≥—Ä–æ–º–Ω—ã–π —Ä–æ—Å—Ç']
            
            result = {
                'probabilities': dict(zip(class_names, probabilities)),
                'predicted_class': class_names[np.argmax(probabilities)],
                'confidence': float(np.max(probabilities)),
                'shooting_star_probability': float(probabilities[2:].sum()),  # –°—Ä–µ–¥–Ω–∏–π+ —Ä–æ—Å—Ç
                'high_growth_probability': float(probabilities[3:].sum()),   # –ë–æ–ª—å—à–æ–π+ —Ä–æ—Å—Ç
                'explosive_growth_probability': float(probabilities[4])      # –û–≥—Ä–æ–º–Ω—ã–π —Ä–æ—Å—Ç
            }
            
            return result
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
            return {}
    
    def save_model(self, filename: str = "shooting_star_model"):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–æ–¥–µ–ª—å –∏ —Å–∫–µ–π–ª–µ—Ä"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            self.model.save(f"{filename}.h5")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∫–µ–π–ª–µ—Ä
            with open(f"{filename}_scaler.pkl", 'wb') as f:
                pickle.dump(self.scaler, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'sequence_length': self.sequence_length,
                'prediction_horizon': self.prediction_horizon,
                'feature_columns': self.feature_columns,
                'trained_at': datetime.now().isoformat()
            }
            
            with open(f"{filename}_metadata.json", 'w') as f:
                json.dump(metadata, f, indent=2)
            
            logger.info(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {filename}")
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–∏: {e}")
    
    def load_model(self, filename: str = "shooting_star_model"):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ —Å–∫–µ–π–ª–µ—Ä"""
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            self.model = tf.keras.models.load_model(f"{filename}.h5")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∫–µ–π–ª–µ—Ä
            with open(f"{filename}_scaler.pkl", 'rb') as f:
                self.scaler = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open(f"{filename}_metadata.json", 'r') as f:
                metadata = json.load(f)
            
            self.sequence_length = metadata['sequence_length']
            self.prediction_horizon = metadata['prediction_horizon']
            self.feature_columns = metadata.get('features', metadata.get('feature_columns', []))
            
            self.is_trained = True
            logger.info(f"üìÇ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {filename}")
            
            return True
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
            return False

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    from data_collector import HistoricalDataCollector
    
    print("üß† –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ï–ô–†–û–ù–ù–û–ô –°–ï–¢–ò –î–õ–Ø –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø –°–¢–†–ï–õ–Ø–Æ–©–ò–• –ú–û–ù–ï–¢")
    print("=" * 70)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ (–µ—Å–ª–∏ –µ—Å—Ç—å)
    collector = HistoricalDataCollector()
    data = collector.load_data("historical_data.json")  # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —É–∂–µ —Å–æ–±—Ä–∞–Ω—ã
    
    if not data:
        print("‚ùå –î–∞–Ω–Ω—ã–µ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ data_collector.py")
        return
    
    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    predictor = ShootingStarPredictor()
    
    # –û–±—É—á–∞–µ–º –Ω–∞ —á–∞—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
    training_data = dict(list(data.items())[:20])  # –ü–µ—Ä–≤—ã–µ 20 –º–æ–Ω–µ—Ç –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    
    success = predictor.train(training_data)
    
    if success:
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –æ–±—É—á–µ–Ω–∞!")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –æ–¥–Ω–æ–π –º–æ–Ω–µ—Ç–µ
        test_symbol = list(data.keys())[0]
        test_df = data[test_symbol]
        
        prediction = predictor.predict(test_df)
        
        if prediction:
            print(f"\nüéØ –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ï –î–õ–Ø {test_symbol}:")
            print(f"   - –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å: {prediction['predicted_class']}")
            print(f"   - –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {prediction['confidence']:.2%}")
            print(f"   - –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å—Ç—Ä–µ–ª—è—é—â–µ–π –º–æ–Ω–µ—Ç—ã: {prediction['shooting_star_probability']:.2%}")
            print(f"   - –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤—ã—Å–æ–∫–æ–≥–æ —Ä–æ—Å—Ç–∞: {prediction['high_growth_probability']:.2%}")
            print(f"   - –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –≤–∑—Ä—ã–≤–Ω–æ–≥–æ —Ä–æ—Å—Ç–∞: {prediction['explosive_growth_probability']:.2%}")
    
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å")

if __name__ == "__main__":
    main()

