#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üî∫ –†–ï–ê–õ–¨–ù–´–ô –¢–†–ï–ù–ï–† –ú–ê–ö–°–ò–ú–£–ú–û–í
–û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ç–æ—á–µ–∫ –≤—ã—Ö–æ–¥–∞ –∏–∑ LONG –ø–æ–∑–∏—Ü–∏–π
"""

import ccxt
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import time
import pickle
import json
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

class RealMaximumTrainer:
    def __init__(self):
        self.symbols = ['BTC/USDT', 'ETH/USDT', 'ADA/USDT', 'XRP/USDT', 'SOL/USDT']
        self.maximums_database = []
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ –º–∞–∫—Å–∏–º—É–º–æ–≤
        self.min_lookback = 6
        self.min_drop_threshold = 2.0  # –ú–∏–Ω–∏–º—É–º 2% –ø–∞–¥–µ–Ω–∏–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.max_duration = 48  # –ú–∞–∫—Å–∏–º—É–º 48 —á–∞—Å–æ–≤ –¥–æ –º–∏–Ω–∏–º—É–º–∞
        self.verification_hours = 6  # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —á–µ—Ä–µ–∑ 6 —á–∞—Å–æ–≤
        
        print("üî∫ –†–ï–ê–õ–¨–ù–´–ô –¢–†–ï–ù–ï–† –ú–ê–ö–°–ò–ú–£–ú–û–í")
        print("üìä –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—ã—Ö–æ–¥–∞ –∏–∑ LONG")
        print("‚è∞ –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")
        print("=" * 50)
        
    def get_recent_data(self, symbol: str, days: int = 30) -> pd.DataFrame:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–Ω–∏"""
        try:
            print(f"üìä –ó–∞–≥—Ä—É–∂–∞—é {symbol} –∑–∞ {days} –¥–Ω–µ–π...")
            exchange = ccxt.binance()
            
            end_time = datetime.now()
            start_time = end_time - timedelta(days=days)
            
            since = int(start_time.timestamp() * 1000)
            end_ts = int(end_time.timestamp() * 1000)
            
            all_ohlcv = []
            current_since = since
            
            while current_since < end_ts:
                try:
                    ohlcv = exchange.fetch_ohlcv(symbol, '1h', since=current_since, limit=1000)
                    if not ohlcv:
                        break
                    
                    all_ohlcv.extend(ohlcv)
                    current_since = ohlcv[-1][0] + 1
                    
                    if current_since > end_ts:
                        break
                        
                    time.sleep(0.1)
                    
                except Exception as e:
                    break
            
            if not all_ohlcv:
                return pd.DataFrame()
            
            all_ohlcv = [candle for candle in all_ohlcv if candle[0] <= end_ts]
            
            df = pd.DataFrame(all_ohlcv, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume'])
            df['timestamp'] = pd.to_datetime(df['timestamp'], unit='ms')
            df = df.set_index('timestamp')
            
            print(f"   ‚úÖ {len(df)} —Å–≤–µ—á–µ–π")
            return df
            
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞: {e}")
            return pd.DataFrame()
    
    def prepare_maximum_features(self, df: pd.DataFrame, max_idx: int) -> dict:
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–∞–∫—Å–∏–º—É–º–∞ (—Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –¥–ª—è –≤—ã—Ö–æ–¥–∞)"""
        try:
            if max_idx < 50 or max_idx >= len(df) - 6:
                return None
            
            # –î–æ–±–∞–≤–ª—è–µ–º EMA
            df['ema_20'] = df['close'].ewm(span=20).mean()
            df['ema_50'] = df['close'].ewm(span=50).mean()
            df['ema_100'] = df['close'].ewm(span=100).mean()
            
            current = df.iloc[max_idx]
            prev = df.iloc[max_idx - 1]
            prev_2 = df.iloc[max_idx - 2]
            
            # üî∫ –ö–õ–Æ–ß–ï–í–´–ï –ü–†–ò–ó–ù–ê–ö–ò –î–õ–Ø –ú–ê–ö–°–ò–ú–£–ú–û–í
            
            # 1. –°–∫–æ—Ä–æ—Å—Ç–∏ (–Ω–∞ –º–∞–∫—Å–∏–º—É–º–µ)
            price_velocity = (current['close'] - prev['close']) / prev['close']
            ema20_velocity = (current['ema_20'] - prev['ema_20']) / prev['ema_20']
            ema50_velocity = (current['ema_50'] - prev['ema_50']) / prev['ema_50']
            
            # 2. –£—Å–∫–æ—Ä–µ–Ω–∏–µ (–∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º—É–º–æ–≤!)
            prev_price_velocity = (prev['close'] - prev_2['close']) / prev_2['close']
            price_acceleration = price_velocity - prev_price_velocity
            
            # 3. –†–∞—Å—Å—Ç–æ—è–Ω–∏—è (—Ü–µ–Ω–∞ –í–´–®–ï EMA - –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å)
            price_distance_ema20 = (current['close'] - current['ema_20']) / current['ema_20']
            price_distance_ema50 = (current['close'] - current['ema_50']) / current['ema_50']
            price_distance_ema100 = (current['close'] - current['ema_100']) / current['ema_100']
            
            # 4. –£–≥–ª—ã —Ç—Ä–µ–Ω–¥–∞ (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ = –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å)
            ema20_angle = ema20_velocity * 100
            ema50_angle = ema50_velocity * 100
            
            # 5. –û–±—ä–µ–º (–≤—ã—Å–æ–∫–∏–π –æ–±—ä–µ–º –Ω–∞ –º–∞–∫—Å–∏–º—É–º–µ = —Å–ª–∞–±–æ—Å—Ç—å)
            avg_volume = df['volume'].iloc[max_idx-20:max_idx].mean()
            volume_ratio = current['volume'] / avg_volume if avg_volume > 0 else 1
            
            # 6. –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (–≤—ã—Å–æ–∫–∞—è –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å = –Ω–µ—Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å)
            volatility = df['close'].iloc[max_idx-20:max_idx].std() / df['close'].iloc[max_idx-20:max_idx].mean()
            
            # 7. –ò–∑–º–µ–Ω–µ–Ω–∏—è —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–π (—É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –ª–∏ –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å)
            prev_dist_ema20 = (prev['close'] - prev['ema_20']) / prev['ema_20']
            distance_change_ema20 = price_distance_ema20 - prev_dist_ema20
            
            # 8. RSI-–ø–æ–¥–æ–±–Ω—ã–π –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä (–ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å)
            price_highs = df['high'].iloc[max_idx-14:max_idx+1]
            price_lows = df['low'].iloc[max_idx-14:max_idx+1]
            avg_gain = price_highs.diff().clip(lower=0).mean()
            avg_loss = (-price_lows.diff()).clip(lower=0).mean()
            rsi_like = 100 - (100 / (1 + (avg_gain / avg_loss if avg_loss > 0 else 1)))
            
            # 9. –°–∏–ª–∞ —Ç—Ä–µ–Ω–¥–∞ (–Ω–∞—Å–∫–æ–ª—å–∫–æ —Å–∏–ª—å–Ω—ã–π —Ä–æ—Å—Ç)
            trend_strength = abs(price_velocity) * volume_ratio
            
            features = {
                # –°–∫–æ—Ä–æ—Å—Ç–∏
                'price_velocity': price_velocity,
                'ema20_velocity': ema20_velocity,
                'ema50_velocity': ema50_velocity,
                
                # –£—Å–∫–æ—Ä–µ–Ω–∏–µ (–ö–†–ò–¢–ò–ß–ù–û –¥–ª—è –º–∞–∫—Å–∏–º—É–º–æ–≤!)
                'price_acceleration': price_acceleration,
                
                # –†–∞—Å—Å—Ç–æ—è–Ω–∏—è (–ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å)
                'price_distance_ema20': price_distance_ema20,
                'price_distance_ema50': price_distance_ema50,
                'price_distance_ema100': price_distance_ema100,
                
                # –£–≥–ª—ã (–ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω—ã–µ = –ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å)
                'ema20_angle': ema20_angle,
                'ema50_angle': ema50_angle,
                
                # –û–±—ä–µ–º –∏ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
                'volume_ratio': volume_ratio,
                'volatility': volatility,
                
                # –ò–∑–º–µ–Ω–µ–Ω–∏—è
                'distance_change_ema20': distance_change_ema20,
                
                # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
                'rsi_like': rsi_like,
                'trend_strength': trend_strength
            }
            
            return features
            
        except Exception:
            return None
    
    def find_real_maximums(self, df: pd.DataFrame, symbol: str):
        """–ü–æ–∏—Å–∫ —Ä–µ–∞–ª—å–Ω—ã—Ö –º–∞–∫—Å–∏–º—É–º–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π"""
        print(f"üî∫ –ò—â—É —Ä–µ–∞–ª—å–Ω—ã–µ –º–∞–∫—Å–∏–º—É–º—ã –≤ {symbol}...")
        
        maximums_found = 0
        
        for i in range(50, len(df) - 6):
            try:
                current_high = df.iloc[i]['high']
                current_time = df.index[i]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ª–æ–∫–∞–ª—å–Ω—ã–π –º–∞–∫—Å–∏–º—É–º
                is_maximum = True
                for j in range(max(0, i-self.min_lookback), min(len(df), i+self.min_lookback+1)):
                    if j != i and df.iloc[j]['high'] >= current_high:
                        is_maximum = False
                        break
                
                if not is_maximum:
                    continue
                
                # –ò—â–µ–º –º–∏–Ω–∏–º—É–º –≤ –±—É–¥—É—â–µ–º (–ø–∞–¥–µ–Ω–∏–µ –æ—Ç –º–∞–∫—Å–∏–º—É–º–∞)
                min_price = current_high
                min_idx = i
                min_time = current_time
                
                for j in range(i+1, min(len(df), i+49)):
                    if df.iloc[j]['low'] < min_price:
                        min_price = df.iloc[j]['low']
                        min_idx = j
                        min_time = df.index[j]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞–¥–µ–Ω–∏–µ (–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è –ø—Ä–∏–±—ã–ª—å)
                drop_percent = ((min_price - current_high) / current_high) * 100
                duration_hours = (min_time - current_time).total_seconds() / 3600
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º –¥–ª—è –º–∞–∫—Å–∏–º—É–º–æ–≤
                if (drop_percent <= -self.min_drop_threshold and 
                    duration_hours <= self.max_duration):
                    
                    features = self.prepare_maximum_features(df, i)
                    
                    if features:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–∞–∫—Å–∏–º—É–º–∞
                        if (features['price_distance_ema20'] > 0.01 and  # –¶–µ–Ω–∞ –≤—ã—à–µ EMA20 (–ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å)
                            features['volume_ratio'] > 0.5 and  # –û–±—ä–µ–º –Ω–µ —Å–ª–∏—à–∫–æ–º –Ω–∏–∑–∫–∏–π
                            features['ema20_angle'] > 0):  # EMA20 –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤–≤–µ—Ä—Ö (–ø–µ—Ä–µ–∫—É–ø–ª–µ–Ω–Ω–æ—Å—Ç—å)
                            
                            maximum = {
                                'symbol': symbol,
                                'entry_time': current_time,
                                'exit_time': min_time,
                                'entry_price': current_high,  # –í—Ö–æ–¥ –Ω–∞ –º–∞–∫—Å–∏–º—É–º–µ
                                'exit_price': min_price,      # –í—ã—Ö–æ–¥ –Ω–∞ –º–∏–Ω–∏–º—É–º–µ
                                'drop_percent': drop_percent,  # –û—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω–∞—è "–ø—Ä–∏–±—ã–ª—å"
                                'duration_hours': duration_hours,
                                'features': features,
                                'is_profitable_exit': drop_percent <= -3.0,  # –•–æ—Ä–æ—à–∏–π –≤—ã—Ö–æ–¥ = –ø–∞–¥–µ–Ω–∏–µ >3%
                                'data_age_hours': (datetime.now() - current_time).total_seconds() / 3600
                            }
                            
                            self.maximums_database.append(maximum)
                            maximums_found += 1
                            
            except Exception:
                continue
        
        print(f"   ‚úÖ {maximums_found} —Ä–µ–∞–ª—å–Ω—ã—Ö –º–∞–∫—Å–∏–º—É–º–æ–≤")
    
    def collect_real_data(self):
        """–°–±–æ—Ä —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        print("\nüìö –°–ë–û–† –†–ï–ê–õ–¨–ù–´–• –î–ê–ù–ù–´–• –ú–ê–ö–°–ò–ú–£–ú–û–í")
        print("-" * 40)
        
        self.maximums_database = []
        
        for i, symbol in enumerate(self.symbols):
            print(f"[{i+1}/{len(self.symbols)}] {symbol}")
            df = self.get_recent_data(symbol, days=30)
            if not df.empty:
                self.find_real_maximums(df, symbol)
            time.sleep(0.5)
        
        print(f"\nüìä –ò–¢–û–ì–û –ù–ê–ô–î–ï–ù–û: {len(self.maximums_database)} —Ä–µ–∞–ª—å–Ω—ã—Ö –º–∞–∫—Å–∏–º—É–º–æ–≤")
        
        if self.maximums_database:
            drops = [m['drop_percent'] for m in self.maximums_database]
            durations = [m['duration_hours'] for m in self.maximums_database]
            good_exits = [m for m in self.maximums_database if m['is_profitable_exit']]
            
            print(f"üìâ –°—Ä–µ–¥–Ω–µ–µ –ø–∞–¥–µ–Ω–∏–µ: {np.mean(drops):.2f}%")
            print(f"‚è±Ô∏è –°—Ä–µ–¥–Ω—è—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {np.mean(durations):.1f} —á–∞—Å–æ–≤")
            print(f"üéØ –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –ø–∞–¥–µ–Ω–∏–µ: {np.min(drops):.2f}%")
            print(f"‚úÖ –•–æ—Ä–æ—à–∏—Ö –≤—ã—Ö–æ–¥–æ–≤: {len(good_exits)} ({len(good_exits)/len(self.maximums_database)*100:.1f}%)")
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
            symbol_stats = {}
            for maximum in self.maximums_database:
                symbol = maximum['symbol']
                if symbol not in symbol_stats:
                    symbol_stats[symbol] = {'count': 0, 'avg_drop': 0, 'good_exits': 0, 'max_drop': 0}
                symbol_stats[symbol]['count'] += 1
                symbol_stats[symbol]['avg_drop'] += maximum['drop_percent']
                if maximum['is_profitable_exit']:
                    symbol_stats[symbol]['good_exits'] += 1
                if maximum['drop_percent'] < symbol_stats[symbol]['max_drop']:
                    symbol_stats[symbol]['max_drop'] = maximum['drop_percent']
            
            print(f"\nüèÜ –ü–æ —Å–∏–º–≤–æ–ª–∞–º:")
            for symbol, stats in sorted(symbol_stats.items(), key=lambda x: abs(x[1]['avg_drop']/x[1]['count']), reverse=True):
                avg_drop = stats['avg_drop'] / stats['count']
                exit_rate = stats['good_exits'] / stats['count'] * 100
                print(f"   {symbol}: {stats['count']} –º–∞–∫—Å–∏–º—É–º–æ–≤, {avg_drop:.2f}% —Å—Ä–µ–¥–Ω–µ–µ –ø–∞–¥–µ–Ω–∏–µ, {exit_rate:.1f}% —Ö–æ—Ä–æ—à–∏—Ö –≤—ã—Ö–æ–¥–æ–≤")
    
    def analyze_maximum_patterns(self):
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π –º–∞–∫—Å–∏–º—É–º–æ–≤"""
        print("\nüîç –ê–ù–ê–õ–ò–ó –ó–ê–ö–û–ù–û–ú–ï–†–ù–û–°–¢–ï–ô –ú–ê–ö–°–ò–ú–£–ú–û–í")
        print("-" * 40)
        
        if len(self.maximums_database) < 5:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π")
            return
        
        # –ê–Ω–∞–ª–∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        feature_stats = {}
        feature_names = list(self.maximums_database[0]['features'].keys())
        
        for feature_name in feature_names:
            good_exit_values = []
            bad_exit_values = []
            
            for maximum in self.maximums_database:
                value = maximum['features'][feature_name]
                if maximum['is_profitable_exit']:
                    good_exit_values.append(value)
                else:
                    bad_exit_values.append(value)
            
            if good_exit_values and bad_exit_values:
                feature_stats[feature_name] = {
                    'good_exit_mean': np.mean(good_exit_values),
                    'good_exit_std': np.std(good_exit_values),
                    'bad_exit_mean': np.mean(bad_exit_values),
                    'bad_exit_std': np.std(bad_exit_values),
                    'difference': abs(np.mean(good_exit_values) - np.mean(bad_exit_values))
                }
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Ä–∞–∑–Ω–æ—Å—Ç–∏ (–≤–∞–∂–Ω–æ—Å—Ç–∏)
        feature_importance = sorted(feature_stats.items(), key=lambda x: x[1]['difference'], reverse=True)
        
        print("üèÜ –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í (–ø–æ —Ä–∞–∑–Ω–æ—Å—Ç–∏ –º–µ–∂–¥—É —Ö–æ—Ä–æ—à–∏–º–∏ –∏ –ø–ª–æ—Ö–∏–º–∏ –≤—ã—Ö–æ–¥–∞–º–∏):")
        for feature_name, stats in feature_importance:
            print(f"   {feature_name}:")
            print(f"     –•–æ—Ä–æ—à–∏–µ –≤—ã—Ö–æ–¥—ã: {stats['good_exit_mean']:.4f} ¬± {stats['good_exit_std']:.4f}")
            print(f"     –ü–ª–æ—Ö–∏–µ –≤—ã—Ö–æ–¥—ã: {stats['bad_exit_mean']:.4f} ¬± {stats['bad_exit_std']:.4f}")
            print(f"     –†–∞–∑–Ω–æ—Å—Ç—å: {stats['difference']:.4f}")
            print()
        
        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏
        print("‚è∞ –í–†–ï–ú–ï–ù–ù–´–ï –ó–ê–ö–û–ù–û–ú–ï–†–ù–û–°–¢–ò:")
        
        # –ü–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
        day_stats = {}
        for maximum in self.maximums_database:
            day = maximum['entry_time'].strftime('%A')
            if day not in day_stats:
                day_stats[day] = {'count': 0, 'total_drop': 0, 'good_exits': 0}
            day_stats[day]['count'] += 1
            day_stats[day]['total_drop'] += maximum['drop_percent']
            if maximum['is_profitable_exit']:
                day_stats[day]['good_exits'] += 1
        
        print("   –ü–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏:")
        for day, stats in sorted(day_stats.items(), key=lambda x: abs(x[1]['total_drop']/x[1]['count']), reverse=True):
            avg_drop = stats['total_drop'] / stats['count']
            exit_rate = stats['good_exits'] / stats['count'] * 100
            print(f"     {day}: {stats['count']} –º–∞–∫—Å–∏–º—É–º–æ–≤, {avg_drop:.2f}% —Å—Ä–µ–¥–Ω–µ–µ –ø–∞–¥–µ–Ω–∏–µ, {exit_rate:.1f}% —Ö–æ—Ä–æ—à–∏—Ö –≤—ã—Ö–æ–¥–æ–≤")
    
    def train_maximum_model(self):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –º–∞–∫—Å–∏–º—É–º–æ–≤"""
        print("\nüß† –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø –ú–ê–ö–°–ò–ú–£–ú–û–í")
        print("-" * 40)
        
        if len(self.maximums_database) < 10:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏")
            print(f"   –ù–∞–π–¥–µ–Ω–æ: {len(self.maximums_database)} –º–∞–∫—Å–∏–º—É–º–æ–≤")
            print(f"   –ù—É–∂–Ω–æ –º–∏–Ω–∏–º—É–º: 10 –º–∞–∫—Å–∏–º—É–º–æ–≤")
            return None
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        X = []
        y = []
        
        feature_names = list(self.maximums_database[0]['features'].keys())
        
        for maximum in self.maximums_database:
            features_list = [maximum['features'][name] for name in feature_names]
            X.append(features_list)
            y.append(1 if maximum['is_profitable_exit'] else 0)
        
        X = np.array(X)
        y = np.array(y)
        
        print(f"üìä –î–∞–Ω–Ω—ã—Ö: {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤, {len(feature_names)} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤")
        print(f"‚úÖ –•–æ—Ä–æ—à–∏—Ö –≤—ã—Ö–æ–¥–æ–≤: {sum(y)} ({sum(y)/len(y)*100:.1f}%)")
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=8)
        model.fit(X, y)
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        importances = model.feature_importances_
        feature_importance = list(zip(feature_names, importances))
        feature_importance.sort(key=lambda x: x[1], reverse=True)
        
        print(f"\nüèÜ –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í (ML –º–æ–¥–µ–ª—å –¥–ª—è –º–∞–∫—Å–∏–º—É–º–æ–≤):")
        for name, importance in feature_importance:
            print(f"   {name}: {importance:.4f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_filename = f'real_maximum_model_{timestamp}.pkl'
        feature_filename = f'real_maximum_features_{timestamp}.pkl'
        
        with open(model_filename, 'wb') as f:
            pickle.dump(model, f)
        
        with open(feature_filename, 'wb') as f:
            pickle.dump(feature_names, f)
        
        print(f"\nüíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_filename}")
        print(f"üíæ –ü—Ä–∏–∑–Ω–∞–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {feature_filename}")
        
        return model, feature_names
    
    def run_maximum_analysis(self):
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –º–∞–∫—Å–∏–º—É–º–æ–≤"""
        print("üöÄ –ê–ù–ê–õ–ò–ó –ú–ê–ö–°–ò–ú–£–ú–û–í –î–õ–Ø –í–´–•–û–î–ê –ò–ó LONG")
        print("=" * 50)
        
        # 1. –°–±–æ—Ä —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.collect_real_data()
        
        if not self.maximums_database:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ")
            return
        
        # 2. –ê–Ω–∞–ª–∏–∑ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π
        self.analyze_maximum_patterns()
        
        # 3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model_result = self.train_maximum_model()
        
        # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –±–∞–∑—É –º–∞–∫—Å–∏–º—É–º–æ–≤
        maximums_for_json = []
        for maximum in self.maximums_database:
            maximum_copy = maximum.copy()
            maximum_copy['entry_time'] = maximum['entry_time'].isoformat()
            maximum_copy['exit_time'] = maximum['exit_time'].isoformat()
            maximums_for_json.append(maximum_copy)
        
        with open(f'real_maximums_{timestamp}.json', 'w', encoding='utf-8') as f:
            json.dump(maximums_for_json, f, ensure_ascii=False, indent=2)
        
        print(f"\n‚úÖ –ê–ù–ê–õ–ò–ó –ú–ê–ö–°–ò–ú–£–ú–û–í –ó–ê–í–ï–†–®–ï–ù!")
        print(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(self.maximums_database)} –º–∞–∫—Å–∏–º—É–º–æ–≤")
        print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ real_maximums_{timestamp}.json")
        
        if model_result:
            print(f"üß† –ú–æ–¥–µ–ª—å –¥–ª—è –º–∞–∫—Å–∏–º—É–º–æ–≤ –æ–±—É—á–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é")
        else:
            print(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞ (–Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö)")

if __name__ == "__main__":
    trainer = RealMaximumTrainer()
    trainer.run_maximum_analysis()
