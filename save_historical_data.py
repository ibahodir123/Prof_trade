#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–• –î–ê–ù–ù–´–•
–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö —Å–æ–±—Ä–∞–Ω–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤ –∏ –º–∞–∫—Å–∏–º—É–º–æ–≤ –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
"""

import json
import pickle
import pandas as pd
from datetime import datetime
import os

class HistoricalDataSaver:
    def __init__(self):
        self.timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–• –î–ê–ù–ù–´–•")
        print("=" * 50)
    
    def load_historical_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π –∏ –¥–∞–Ω–Ω—ã—Ö"""
        print("üìÇ –ó–∞–≥—Ä—É–∂–∞—é –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –º–æ–¥–µ–ª–∏...")
        
        try:
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
            with open('historical_minimum_model_20250923_062225.pkl', 'rb') as f:
                self.minimum_model = pickle.load(f)
            
            with open('historical_maximum_model_20250923_062225.pkl', 'rb') as f:
                self.maximum_model = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫–∏
            with open('historical_minimum_scaler_20250923_062225.pkl', 'rb') as f:
                self.minimum_scaler = pickle.load(f)
            
            with open('historical_maximum_scaler_20250923_062225.pkl', 'rb') as f:
                self.maximum_scaler = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            with open('historical_minimum_features_20250923_062225.pkl', 'rb') as f:
                self.minimum_features = pickle.load(f)
            
            with open('historical_maximum_features_20250923_062225.pkl', 'rb') as f:
                self.maximum_features = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            with open('historical_training_metadata_20250923_062225.json', 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            
            print("   ‚úÖ –í—Å–µ –º–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
            return True
            
        except FileNotFoundError as e:
            print(f"   ‚ùå –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {e}")
            return False
        except Exception as e:
            print(f"   ‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
            return False
    
    def create_models_directory(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –º–æ–¥–µ–ª–µ–π"""
        models_dir = "historical_models"
        if not os.path.exists(models_dir):
            os.makedirs(models_dir)
            print(f"üìÅ –°–æ–∑–¥–∞–Ω–∞ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {models_dir}")
        return models_dir
    
    def save_models_to_directory(self, models_dir):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é"""
        print(f"\nüíæ –°–æ—Ö—Ä–∞–Ω—è—é –º–æ–¥–µ–ª–∏ –≤ {models_dir}/")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        with open(f'{models_dir}/minimum_model.pkl', 'wb') as f:
            pickle.dump(self.minimum_model, f)
        
        with open(f'{models_dir}/maximum_model.pkl', 'wb') as f:
            pickle.dump(self.maximum_model, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫–∏
        with open(f'{models_dir}/minimum_scaler.pkl', 'wb') as f:
            pickle.dump(self.minimum_scaler, f)
        
        with open(f'{models_dir}/maximum_scaler.pkl', 'wb') as f:
            pickle.dump(self.maximum_scaler, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        with open(f'{models_dir}/minimum_features.pkl', 'wb') as f:
            pickle.dump(self.minimum_features, f)
        
        with open(f'{models_dir}/maximum_features.pkl', 'wb') as f:
            pickle.dump(self.maximum_features, f)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        with open(f'{models_dir}/training_metadata.json', 'w', encoding='utf-8') as f:
            json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        
        print("   ‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é")
    
    def create_model_info(self, models_dir):
        """–°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞ –æ –º–æ–¥–µ–ª—è—Ö"""
        print(f"\nüìã –°–æ–∑–¥–∞—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª...")
        
        model_info = {
            "model_info": {
                "training_period": "2017-2024",
                "testing_period": "2025",
                "symbols": ["BTC/USDT", "ETH/USDT", "ADA/USDT", "XRP/USDT", "SOL/USDT"],
                "minimums_count": 14026,
                "maximums_count": 13860,
                "minimum_accuracy": "89.7%",
                "maximum_accuracy": "89.4%",
                "average_profit": "9.80%",
                "average_drop": "-8.56%"
            },
            "files": {
                "minimum_model": "minimum_model.pkl - –ú–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–∏–Ω–∏–º—É–º–æ–≤ (–≤—Ö–æ–¥ –≤ LONG)",
                "maximum_model": "maximum_model.pkl - –ú–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–∞–∫—Å–∏–º—É–º–æ–≤ (–≤—ã—Ö–æ–¥ –∏–∑ LONG)",
                "minimum_scaler": "minimum_scaler.pkl - –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–∏–Ω–∏–º—É–º–æ–≤",
                "maximum_scaler": "maximum_scaler.pkl - –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–∞–∫—Å–∏–º—É–º–æ–≤",
                "minimum_features": "minimum_features.pkl - –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –º–∏–Ω–∏–º—É–º–æ–≤",
                "maximum_features": "maximum_features.pkl - –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –º–∞–∫—Å–∏–º—É–º–æ–≤",
                "training_metadata": "training_metadata.json - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è"
            },
            "usage": {
                "step_1": "–ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫–∏",
                "step_2": "–ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏–∑ feature_names",
                "step_3": "–ü—Ä–∏–º–µ–Ω–∏—Ç–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫ –∫ –¥–∞–Ω–Ω—ã–º",
                "step_4": "–ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è",
                "step_5": "–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–π—Ç–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (1 = —Ö–æ—Ä–æ—à–∏–π —Å–∏–≥–Ω–∞–ª, 0 = –ø–ª–æ—Ö–æ–π —Å–∏–≥–Ω–∞–ª)"
            },
            "features": {
                "minimum_features": self.minimum_features,
                "maximum_features": self.maximum_features
            },
            "created_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "version": "1.0"
        }
        
        with open(f'{models_dir}/MODEL_INFO.json', 'w', encoding='utf-8') as f:
            json.dump(model_info, f, ensure_ascii=False, indent=2)
        
        print("   ‚úÖ –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: MODEL_INFO.json")
    
    def create_usage_example(self, models_dir):
        """–°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        print(f"\nüìù –°–æ–∑–¥–∞—é –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è...")
        
        example_code = '''#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
–ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–• –ú–û–î–ï–õ–ï–ô
"""

import pickle
import numpy as np
import pandas as pd
from datetime import datetime

class HistoricalModelPredictor:
    def __init__(self, models_dir="historical_models"):
        self.models_dir = models_dir
        self.load_models()
    
    def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π"""
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
        with open(f'{self.models_dir}/minimum_model.pkl', 'rb') as f:
            self.minimum_model = pickle.load(f)
        
        with open(f'{self.models_dir}/maximum_model.pkl', 'rb') as f:
            self.maximum_model = pickle.load(f)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫–∏
        with open(f'{self.models_dir}/minimum_scaler.pkl', 'rb') as f:
            self.minimum_scaler = pickle.load(f)
        
        with open(f'{self.models_dir}/maximum_scaler.pkl', 'rb') as f:
            self.maximum_scaler = pickle.load(f)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
        with open(f'{self.models_dir}/minimum_features.pkl', 'rb') as f:
            self.minimum_features = pickle.load(f)
        
        with open(f'{self.models_dir}/maximum_features.pkl', 'rb') as f:
            self.maximum_features = pickle.load(f)
        
        print("‚úÖ –ú–æ–¥–µ–ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
    
    def prepare_minimum_features(self, df, idx):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–∏–Ω–∏–º—É–º–∞"""
        if idx < 50 or idx >= len(df) - 6:
            return None
        
        current = df.iloc[idx]
        prev = df.iloc[idx - 1]
        
        features = {
            'price_velocity': (current['close'] - prev['close']) / prev['close'],
            'ema20_velocity': (current['ema_20'] - prev['ema_20']) / prev['ema_20'],
            'ema50_velocity': (current['ema_50'] - prev['ema_50']) / prev['ema_50'],
            'ema100_velocity': (current['ema_100'] - prev['ema_100']) / prev['ema_100'],
            'price_distance_ema20': (current['close'] - current['ema_20']) / current['ema_20'],
            'price_distance_ema50': (current['close'] - current['ema_50']) / current['ema_50'],
            'price_distance_ema100': (current['close'] - current['ema_100']) / current['ema_100'],
            'ema20_angle': ((current['ema_20'] - prev['ema_20']) / prev['ema_20']) * 100,
            'ema50_angle': ((current['ema_50'] - prev['ema_50']) / prev['ema_50']) * 100,
            'volatility': df['close'].iloc[idx-20:idx].std() / df['close'].iloc[idx-20:idx].mean(),
            'volume_ratio': current['volume'] / df['volume'].iloc[idx-20:idx].mean() if df['volume'].iloc[idx-20:idx].mean() > 0 else 1
        }
        
        return features
    
    def predict_minimum(self, features_dict):
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∏–Ω–∏–º—É–º–∞"""
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        features_list = [features_dict.get(name, 0) for name in self.minimum_features]
        features_array = np.array(features_list).reshape(1, -1)
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º
        features_scaled = self.minimum_scaler.transform(features_array)
        
        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º
        prediction = self.minimum_model.predict(features_scaled)[0]
        probability = self.minimum_model.predict_proba(features_scaled)[0]
        
        return {
            'prediction': prediction,
            'confidence': max(probability),
            'is_good_entry': prediction == 1
        }
    
    def predict_maximum(self, features_dict):
        """–ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –º–∞–∫—Å–∏–º—É–º–∞"""
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        features_list = [features_dict.get(name, 0) for name in self.maximum_features]
        features_array = np.array(features_list).reshape(1, -1)
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º
        features_scaled = self.maximum_scaler.transform(features_array)
        
        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ–º
        prediction = self.maximum_model.predict(features_scaled)[0]
        probability = self.maximum_model.predict_proba(features_scaled)[0]
        
        return {
            'prediction': prediction,
            'confidence': max(probability),
            'is_good_exit': prediction == 1
        }

# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    predictor = HistoricalModelPredictor()
    
    # –ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö (–∑–∞–º–µ–Ω–∏—Ç–µ –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ)
    # df = your_dataframe_with_ema_columns
    
    # –î–ª—è –º–∏–Ω–∏–º—É–º–∞
    # features = predictor.prepare_minimum_features(df, idx)
    # result = predictor.predict_minimum(features)
    # print(f"–ú–∏–Ω–∏–º—É–º: {'–•–æ—Ä–æ—à–∏–π –≤—Ö–æ–¥' if result['is_good_entry'] else '–ü–ª–æ—Ö–æ–π –≤—Ö–æ–¥'} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f})")
    
    # –î–ª—è –º–∞–∫—Å–∏–º—É–º–∞
    # features = predictor.prepare_maximum_features(df, idx)
    # result = predictor.predict_maximum(features)
    # print(f"–ú–∞–∫—Å–∏–º—É–º: {'–•–æ—Ä–æ—à–∏–π –≤—ã—Ö–æ–¥' if result['is_good_exit'] else '–ü–ª–æ—Ö–æ–π –≤—ã—Ö–æ–¥'} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {result['confidence']:.2f})")
'''
        
        with open(f'{models_dir}/usage_example.py', 'w', encoding='utf-8') as f:
            f.write(example_code)
        
        print("   ‚úÖ –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–æ–∑–¥–∞–Ω: usage_example.py")
    
    def create_readme(self, models_dir):
        """–°–æ–∑–¥–∞–Ω–∏–µ README —Ñ–∞–π–ª–∞"""
        print(f"\nüìñ –°–æ–∑–¥–∞—é README —Ñ–∞–π–ª...")
        
        readme_content = '''# üìö –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–ï –ú–û–î–ï–õ–ò –î–õ–Ø –¢–û–†–ì–û–í–õ–ò LONG –ü–û–ó–ò–¶–ò–Ø–ú–ò

## üìä –û–ë–ó–û–†

–≠—Ç–æ—Ç –Ω–∞–±–æ—Ä —Å–æ–¥–µ—Ä–∂–∏—Ç –º–æ–¥–µ–ª–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è, –æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö —Å 2017 –ø–æ 2024 –≥–æ–¥ –¥–ª—è —Ç–æ—Ä–≥–æ–≤–ª–∏ LONG –ø–æ–∑–∏—Ü–∏—è–º–∏.

## üéØ –ù–ê–ó–ù–ê–ß–ï–ù–ò–ï

- **–ú–æ–¥–µ–ª—å –º–∏–Ω–∏–º—É–º–æ–≤**: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –≤—Ö–æ–¥–∞ –≤ LONG –ø–æ–∑–∏—Ü–∏–∏
- **–ú–æ–¥–µ–ª—å –º–∞–∫—Å–∏–º—É–º–æ–≤**: –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–µ —Ç–æ—á–∫–∏ –≤—ã—Ö–æ–¥–∞ –∏–∑ LONG –ø–æ–∑–∏—Ü–∏–π

## üìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –û–ë–£–ß–ï–ù–ò–Ø

- **–ü–µ—Ä–∏–æ–¥ –æ–±—É—á–µ–Ω–∏—è**: 2017-2024 (8 –ª–µ—Ç)
- **–°–∏–º–≤–æ–ª—ã**: BTC/USDT, ETH/USDT, ADA/USDT, XRP/USDT, SOL/USDT
- **–ú–∏–Ω–∏–º—É–º–æ–≤**: 14,026 (89.7% –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö)
- **–ú–∞–∫—Å–∏–º—É–º–æ–≤**: 13,860 (89.4% —Ö–æ—Ä–æ—à–∏—Ö –≤—ã—Ö–æ–¥–æ–≤)
- **–°—Ä–µ–¥–Ω—è—è –ø—Ä–∏–±—ã–ª—å**: 9.80%
- **–°—Ä–µ–¥–Ω–µ–µ –ø–∞–¥–µ–Ω–∏–µ**: -8.56%

## üìÅ –§–ê–ô–õ–´

- `minimum_model.pkl` - –ú–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–∏–Ω–∏–º—É–º–æ–≤
- `maximum_model.pkl` - –ú–æ–¥–µ–ª—å –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–∞–∫—Å–∏–º—É–º–æ–≤
- `minimum_scaler.pkl` - –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–∏–Ω–∏–º—É–º–æ–≤
- `maximum_scaler.pkl` - –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –º–∞–∫—Å–∏–º—É–º–æ–≤
- `minimum_features.pkl` - –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –º–∏–Ω–∏–º—É–º–æ–≤
- `maximum_features.pkl` - –°–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–∏ –º–∞–∫—Å–∏–º—É–º–æ–≤
- `training_metadata.json` - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—É—á–µ–Ω–∏—è
- `MODEL_INFO.json` - –ü–æ–¥—Ä–æ–±–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö
- `usage_example.py` - –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- `README.md` - –≠—Ç–æ—Ç —Ñ–∞–π–ª

## üöÄ –ë–´–°–¢–†–´–ô –°–¢–ê–†–¢

1. –ó–∞–≥—Ä—É–∑–∏—Ç–µ –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –æ–¥–Ω—É –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
2. –ò–∑—É—á–∏—Ç–µ `usage_example.py` –¥–ª—è –ø–æ–Ω–∏–º–∞–Ω–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
3. –ü–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –¥–∞–Ω–Ω—ã–µ —Å EMA –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä–∞–º–∏
4. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–æ–¥–µ–ª–∏ –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è

## üîß –¢–†–ï–ë–û–í–ê–ù–ò–Ø

- Python 3.7+
- pandas
- numpy
- scikit-learn
- pickle

## üìä –ü–†–ò–ó–ù–ê–ö–ò

–ú–æ–¥–µ–ª–∏ –∏—Å–ø–æ–ª—å–∑—É—é—Ç —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏:
- –°–∫–æ—Ä–æ—Å—Ç–∏ —Ü–µ–Ω—ã –∏ EMA
- –†–∞—Å—Å—Ç–æ—è–Ω–∏—è –æ—Ç EMA
- –£–≥–ª—ã —Ç—Ä–µ–Ω–¥–∞
- –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å
- –û–±—ä–µ–º
- –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –º–µ–∂–¥—É EMA

## ‚ö†Ô∏è –í–ê–ñ–ù–û

- –ú–æ–¥–µ–ª–∏ –æ–±—É—á–µ–Ω—ã –Ω–∞ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö
- –¢—Ä–µ–±—É–µ—Ç—Å—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
- –ù–µ –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É—é—Ç –ø—Ä–∏–±—ã–ª—å–Ω–æ—Å—Ç—å –≤ –±—É–¥—É—â–µ–º
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Å –æ—Å—Ç–æ—Ä–æ–∂–Ω–æ—Å—Ç—å—é –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ç–æ—Ä–≥–æ–≤–ª–µ

## üìû –ü–û–î–î–ï–†–ñ–ö–ê

–î–ª—è –≤–æ–ø—Ä–æ—Å–æ–≤ –∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –æ–±—Ä–∞—â–∞–π—Ç–µ—Å—å –∫ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫—É —Å–∏—Å—Ç–µ–º—ã.

---
*–°–æ–∑–¥–∞–Ω–æ: 2025-01-23*
*–í–µ—Ä—Å–∏—è: 1.0*
'''
        
        with open(f'{models_dir}/README.md', 'w', encoding='utf-8') as f:
            f.write(readme_content)
        
        print("   ‚úÖ README —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: README.md")
    
    def run_save_process(self):
        """–ó–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è"""
        print("üöÄ –ó–ê–ü–£–°–ö –°–û–•–†–ê–ù–ï–ù–ò–Ø –ò–°–¢–û–†–ò–ß–ï–°–ö–ò–• –î–ê–ù–ù–´–•")
        print("=" * 50)
        
        # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
        if not self.load_historical_models():
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª–∏")
            return
        
        # 2. –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        models_dir = self.create_models_directory()
        
        # 3. –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª–∏
        self.save_models_to_directory(models_dir)
        
        # 4. –°–æ–∑–¥–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª
        self.create_model_info(models_dir)
        
        # 5. –°–æ–∑–¥–∞–µ–º –ø—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.create_usage_example(models_dir)
        
        # 6. –°–æ–∑–¥–∞–µ–º README
        self.create_readme(models_dir)
        
        print(f"\n‚úÖ –°–û–•–†–ê–ù–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
        print(f"üìÅ –í—Å–µ —Ñ–∞–π–ª—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é: {models_dir}/")
        print(f"üìä –ú–æ–¥–µ–ª–∏ –≥–æ—Ç–æ–≤—ã –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞")
        print(f"üöÄ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ usage_example.py –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—ã")

if __name__ == "__main__":
    saver = HistoricalDataSaver()
    saver.run_save_process()
