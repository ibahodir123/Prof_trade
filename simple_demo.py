#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
"""
import os
import json
import numpy as np
import pandas as pd
import tensorflow as tf
import pickle

def load_simple_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ—Å—Ç—É—é –º–æ–¥–µ–ª—å"""
    print("üîÑ –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å...")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model = tf.keras.models.load_model("simple_shooting_star_model.h5")
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–∫–µ–π–ª–µ—Ä
    with open("simple_shooting_star_scaler.pkl", 'rb') as f:
        scaler = pickle.load(f)
    print("‚úÖ –°–∫–µ–π–ª–µ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω!")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    with open("simple_shooting_star_metadata.json", 'r') as f:
        metadata = json.load(f)
    print("‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
    
    return model, scaler, metadata

def create_test_data():
    """–°–æ–∑–¥–∞–µ—Ç —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ"""
    print("üìä –°–æ–∑–¥–∞—é —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ...")
    
    # –°–æ–∑–¥–∞–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    np.random.seed(42)
    n_samples = 100
    
    data = {
        'close': np.random.randn(n_samples).cumsum() + 100,
        'volume': np.random.exponential(1000, n_samples),
        'high': np.random.randn(n_samples).cumsum() + 102,
        'low': np.random.randn(n_samples).cumsum() + 98,
        'rsi': np.random.uniform(20, 80, n_samples)
    }
    
    df = pd.DataFrame(data)
    return df

def prepare_features(df):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏"""
    features = pd.DataFrame()
    features['close'] = df['close']
    features['volume'] = df['volume']
    features['high'] = df['high']
    features['low'] = df['low']
    features['rsi'] = df['rsi']
    
    return features.fillna(0)

def create_sequences(features, sequence_length=12):
    """–°–æ–∑–¥–∞–µ—Ç –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –¥–ª—è LSTM"""
    X = []
    
    for i in range(sequence_length, len(features)):
        X.append(features[i-sequence_length:i].values)
    
    return np.array(X)

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("üß† –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–û–•–†–ê–ù–ï–ù–ù–û–ô –ú–û–î–ï–õ–ò")
    print("=" * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∞–π–ª—ã
    required_files = [
        "simple_shooting_star_model.h5",
        "simple_shooting_star_scaler.pkl",
        "simple_shooting_star_metadata.json"
    ]
    
    missing_files = [f for f in required_files if not os.path.exists(f)]
    
    if missing_files:
        print("‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ñ–∞–π–ª—ã:")
        for file in missing_files:
            print(f"   - {file}")
        print("\nüîß –î–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ:")
        print("   python simple_train.py")
        return
    
    print("‚úÖ –í—Å–µ —Ñ–∞–π–ª—ã –Ω–∞–π–¥–µ–Ω—ã!")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        model, scaler, metadata = load_simple_model()
        
        print(f"\nüìã –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:")
        print(f"   - –î–ª–∏–Ω–∞ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {metadata['sequence_length']}")
        print(f"   - –ì–æ—Ä–∏–∑–æ–Ω—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {metadata['prediction_horizon']}")
        print(f"   - –ü—Ä–∏–∑–Ω–∞–∫–∏: {metadata['features']}")
        print(f"   - –ö–ª–∞—Å—Å—ã: {metadata['classes']}")
        print(f"   - –¢–æ—á–Ω–æ—Å—Ç—å: {metadata['accuracy']:.1%}")
        
        # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
        test_df = create_test_data()
        features = prepare_features(test_df)
        
        print(f"\nüìä –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ: {len(features)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        # –°–æ–∑–¥–∞–µ–º –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        X = create_sequences(features, metadata['sequence_length'])
        print(f"üìà –°–æ–∑–¥–∞–Ω–æ {len(X)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ
        X_scaled = scaler.transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
        
        # –î–µ–ª–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        print("\nüéØ –î–µ–ª–∞—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è...")
        predictions = model.predict(X_scaled, verbose=0)
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        predicted_classes = np.argmax(predictions, axis=1)
        confidences = np.max(predictions, axis=1)
        
        print(f"\nüìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ {len(predictions)} –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π:")
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
        class_counts = np.bincount(predicted_classes)
        class_names = metadata['classes']
        
        for i, count in enumerate(class_counts):
            if count > 0:
                print(f"   {class_names[i]}: {count} ({count/len(predictions):.1%})")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ª—É—á—à–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        best_indices = np.argsort(confidences)[-5:]  # –¢–æ–ø-5 –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
        
        print(f"\nüèÜ –¢–û–ü-5 –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–ô (–ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏):")
        for i, idx in enumerate(reversed(best_indices)):
            pred_class = predicted_classes[idx]
            confidence = confidences[idx]
            class_name = class_names[pred_class]
            
            print(f"   {i+1}. –ö–ª–∞—Å—Å: {class_name}")
            print(f"      –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%}")
            print(f"      –í—Å–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {predictions[idx]}")
            print()
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–µ–ª—è—é—â–∏–µ –º–æ–Ω–µ—Ç—ã
        shooting_star_indices = predicted_classes >= 3  # –ö–ª–∞—Å—Å—ã 3 –∏ 4 - –≤—ã—Å–æ–∫–∏–π/–≤–∑—Ä—ã–≤–Ω–æ–π —Ä–æ—Å—Ç
        shooting_star_count = np.sum(shooting_star_indices)
        
        print(f"üöÄ –°–¢–†–ï–õ–Ø–Æ–©–ò–ï –ú–û–ù–ï–¢–´:")
        print(f"   - –ù–∞–π–¥–µ–Ω–æ: {shooting_star_count} ({shooting_star_count/len(predictions):.1%})")
        
        if shooting_star_count > 0:
            print(f"   - –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {np.mean(confidences[shooting_star_indices]):.1%}")
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –¥–µ—Ç–∞–ª–∏ —Å—Ç—Ä–µ–ª—è—é—â–∏—Ö –º–æ–Ω–µ—Ç
            print(f"\nüìà –î–ï–¢–ê–õ–ò –°–¢–†–ï–õ–Ø–Æ–©–ò–• –ú–û–ù–ï–¢:")
            for i, idx in enumerate(np.where(shooting_star_indices)[0][:3]):  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
                pred_class = predicted_classes[idx]
                confidence = confidences[idx]
                class_name = class_names[pred_class]
                
                print(f"   {i+1}. {class_name} (—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {confidence:.1%})")
        
        print("\nüéâ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê!")
        print("\nüí° –í—ã–≤–æ–¥—ã:")
        print("   ‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∏–∑ —Ñ–∞–π–ª–æ–≤")
        print("   ‚úÖ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Ä–∞–±–æ—Ç–∞—é—Ç –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏")
        print("   ‚úÖ –°–∏—Å—Ç–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è/–∑–∞–≥—Ä—É–∑–∫–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç")
        print("   ‚úÖ –ù–µ –Ω—É–∂–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–∞—Ç—å –∫–∞–∂–¥—ã–π —Ä–∞–∑")
        
        print("\nüìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:")
        print("   - python simple_demo.py (—ç—Ç–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è)")
        print("   - python shooting_star_bot.py (–∑–∞–ø—É—Å–∫ –±–æ—Ç–∞)")
        print("   - python simple_train.py (–ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ)")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

