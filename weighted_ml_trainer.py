#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ML –¢—Ä–µ–Ω–µ—Ä —Å –≤–∑–≤–µ—à–µ–Ω–Ω–æ–π –æ—Ü–µ–Ω–∫–æ–π –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤ –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –º–∏–Ω–∏–º—É–º–æ–≤
–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —É–¥–µ–ª—å–Ω—ã–π –≤–µ—Å –∫–∞–∂–¥–æ–≥–æ –∏–∑ 4 –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
"""

import json
import numpy as np
import pandas as pd
from datetime import datetime
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import pickle
import warnings
warnings.filterwarnings('ignore')

class WeightedMLTrainer:
    def __init__(self):
        self.minimums_data = []
        self.model = None
        self.feature_names = ['price_velocity', 'ema20_velocity', 'ema20_angle', 'distance_to_ema20']
        self.feature_weights = {}
        
    def load_minimums_data(self, filename: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –æ –º–∏–Ω–∏–º—É–º–∞—Ö"""
        try:
            print(f"üìä –ó–∞–≥—Ä—É–∂–∞—é –¥–∞–Ω–Ω—ã–µ –∏–∑ {filename}")
            
            with open(filename, 'r', encoding='utf-8') as f:
                self.minimums_data = json.load(f)
            
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.minimums_data)} –º–∏–Ω–∏–º—É–º–æ–≤")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
            return False
    
    def analyze_feature_importance(self):
        """–ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –∫–∞–∂–¥–æ–≥–æ –∫—Ä–∏—Ç–µ—Ä–∏—è"""
        print("\\nüîç –ê–ù–ê–õ–ò–ó –í–ê–ñ–ù–û–°–¢–ò –ö–†–ò–¢–ï–†–ò–ï–í")
        print("=" * 40)
        
        if not self.minimums_data:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞!")
            return
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –ø—Ä–∏–±—ã–ª—å–Ω—ã–µ –∏ —É–±—ã—Ç–æ—á–Ω—ã–µ –º–∏–Ω–∏–º—É–º—ã
        profitable = [m for m in self.minimums_data if m['is_profitable']]
        unprofitable = [m for m in self.minimums_data if not m['is_profitable']]
        
        print(f"üìà –ü—Ä–∏–±—ã–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤: {len(profitable)}")
        print(f"üìâ –£–±—ã—Ç–æ—á–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤: {len(unprofitable)}")
        
        if len(profitable) == 0 or len(unprofitable) == 0:
            print("‚ùå –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏!")
            return
        
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π
        feature_scores = {}
        
        for feature in self.feature_names:
            # –ó–Ω–∞—á–µ–Ω–∏—è –∫—Ä–∏—Ç–µ—Ä–∏—è –¥–ª—è –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤
            profitable_values = [m['criteria'][feature] for m in profitable]
            unprofitable_values = [m['criteria'][feature] for m in unprofitable]
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            prof_mean = np.mean(profitable_values)
            prof_std = np.std(profitable_values)
            unprof_mean = np.mean(unprofitable_values)
            unprof_std = np.std(unprofitable_values)
            
            # –†–∞—Å—á–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ (—á–µ–º –±–æ–ª—å—à–µ —Ä–∞–∑–Ω–∏—Ü–∞ —Å—Ä–µ–¥–Ω–∏—Ö, —Ç–µ–º –≤–∞–∂–Ω–µ–µ –∫—Ä–∏—Ç–µ—Ä–∏–π)
            separation_power = abs(prof_mean - unprof_mean) / (prof_std + unprof_std + 0.001)
            
            feature_scores[feature] = {
                'separation_power': separation_power,
                'profitable_mean': prof_mean,
                'profitable_std': prof_std,
                'unprofitable_mean': unprof_mean,
                'unprofitable_std': unprof_std
            }
            
            print(f"\\nüìä {feature}:")
            print(f"   –ü—Ä–∏–±—ã–ª—å–Ω—ã–µ: {prof_mean:.3f} ¬± {prof_std:.3f}")
            print(f"   –£–±—ã—Ç–æ—á–Ω—ã–µ: {unprof_mean:.3f} ¬± {unprof_std:.3f}")
            print(f"   –†–∞–∑–¥–µ–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å–∏–ª–∞: {separation_power:.3f}")
        
        # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –≤–µ—Å–∞
        total_power = sum([scores['separation_power'] for scores in feature_scores.values()])
        
        print(f"\\n‚öñÔ∏è –£–î–ï–õ–¨–ù–´–ï –í–ï–°–ê –ö–†–ò–¢–ï–†–ò–ï–í:")
        for feature in self.feature_names:
            weight = feature_scores[feature]['separation_power'] / total_power
            self.feature_weights[feature] = weight
            print(f"   {feature}: {weight:.1%}")
        
        return feature_scores
    
    def prepare_training_data(self):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
        if not self.minimums_data:
            return None, None
        
        X = []
        y = []
        
        for minimum in self.minimums_data:
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏
            features = [minimum['criteria'][name] for name in self.feature_names]
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ NaN/Inf
            if any(np.isnan(f) or np.isinf(f) for f in features):
                continue
            
            X.append(features)
            
            # –ú–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞ (1 = –ø—Ä–∏–±—ã–ª—å–Ω—ã–π –º–∏–Ω–∏–º—É–º, 0 = —É–±—ã—Ç–æ—á–Ω—ã–π)
            y.append(1 if minimum['is_profitable'] else 0)
        
        return np.array(X), np.array(y)
    
    def train_model(self):
        """–û–±—É—á–µ–Ω–∏–µ ML –º–æ–¥–µ–ª–∏"""
        print("\\nüß† –û–ë–£–ß–ï–ù–ò–ï ML –ú–û–î–ï–õ–ò")
        print("=" * 30)
        
        X, y = self.prepare_training_data()
        if X is None or len(X) == 0:
            print("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!")
            return False
        
        print(f"üìä –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(X)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        print(f"üìà –ò–∑ –Ω–∏—Ö –ø—Ä–∏–±—ã–ª—å–Ω—ã—Ö: {sum(y)}")
        print(f"üìâ –£–±—ã—Ç–æ—á–Ω—ã—Ö: {len(y) - sum(y)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤
        if sum(y) == 0 or sum(y) == len(y):
            print("‚ùå –í—Å–µ –ø—Ä–∏–º–µ—Ä—ã –æ–¥–Ω–æ–≥–æ –∫–ª–∞—Å—Å–∞! –ù–µ–ª—å–∑—è –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å.")
            return False
        
        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ train/test
        if len(X) < 10:
            print("‚ö†Ô∏è –ú–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è")
            X_train, X_test, y_train, y_test = X, X, y, y
        else:
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        self.model = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2
        )
        
        self.model.fit(X_train, y_train)
        
        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        train_pred = self.model.predict(X_train)
        test_pred = self.model.predict(X_test)
        
        train_acc = accuracy_score(y_train, train_pred)
        test_acc = accuracy_score(y_test, test_pred)
        
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏: {train_acc:.3f}")
        print(f"‚úÖ –¢–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–µ: {test_acc:.3f}")
        
        # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ—Ç –º–æ–¥–µ–ª–∏
        model_importances = self.model.feature_importances_
        print(f"\\nüèÜ –í–ê–ñ–ù–û–°–¢–¨ –ü–†–ò–ó–ù–ê–ö–û–í (–ø–æ –º–æ–¥–µ–ª–∏):")
        for i, feature in enumerate(self.feature_names):
            print(f"   {feature}: {model_importances[i]:.3f}")
        
        # –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
        if len(X_test) > 0:
            cm = confusion_matrix(y_test, test_pred)
            print(f"\\nüìä –ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫:")
            print(f"   Predicted:  [0, 1]")
            print(f"   Actual 0:   {cm[0]}")
            print(f"   Actual 1:   {cm[1]}")
        
        return True
    
    def calculate_minimum_probability(self, criteria_values: dict) -> dict:
        """–†–∞—Å—á–µ—Ç –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –º–∏–Ω–∏–º—É–º–∞ –ø–æ –∫—Ä–∏—Ç–µ—Ä–∏—è–º"""
        if not self.model:
            return {'error': '–ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞'}
        
        try:
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤–µ–∫—Ç–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            feature_vector = []
            for name in self.feature_names:
                value = criteria_values.get(name, 0.0)
                if np.isnan(value) or np.isinf(value):
                    value = 0.0
                feature_vector.append(value)
            
            feature_vector = np.array(feature_vector).reshape(1, -1)
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            prediction = self.model.predict(feature_vector)[0]
            probabilities = self.model.predict_proba(feature_vector)[0]
            
            # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å –≤–µ—Å–∞)
            weighted_score = 0.0
            if self.feature_weights:
                for i, feature in enumerate(self.feature_names):
                    weight = self.feature_weights[feature]
                    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∑–Ω–∞—á–µ–Ω–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏—è –¥–ª—è –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏—è
                    normalized_value = max(0, min(1, (abs(feature_vector[0][i]) / 10)))  # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–∞—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
                    weighted_score += weight * normalized_value
            
            result = {
                'is_minimum': bool(prediction),
                'probability': float(probabilities[1]),  # –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∞ 1 (–ø—Ä–∏–±—ã–ª—å–Ω—ã–π –º–∏–Ω–∏–º—É–º)
                'weighted_score': weighted_score,
                'criteria_analysis': {}
            }
            
            # –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
            for i, feature in enumerate(self.feature_names):
                result['criteria_analysis'][feature] = {
                    'value': float(feature_vector[0][i]),
                    'weight': self.feature_weights.get(feature, 0.25),
                    'contribution': self.feature_weights.get(feature, 0.25) * abs(feature_vector[0][i])
                }
            
            return result
            
        except Exception as e:
            return {'error': f'–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}'}
    
    def save_model(self, model_filename: str = "minimum_detector_model.pkl"):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        if not self.model:
            print("‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –æ–±—É—á–µ–Ω–∞!")
            return False
        
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            with open(model_filename, 'wb') as f:
                pickle.dump(self.model, f)
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–µ—Å–∞ –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            metadata = {
                'feature_names': self.feature_names,
                'feature_weights': self.feature_weights,
                'training_date': datetime.now().isoformat(),
                'total_minimums': len(self.minimums_data)
            }
            
            metadata_filename = model_filename.replace('.pkl', '_metadata.json')
            with open(metadata_filename, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            print(f"üíæ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {model_filename}")
            print(f"üíæ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {metadata_filename}")
            return True
            
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è: {e}")
            return False
    
    def test_model_on_examples(self):
        """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö"""
        if not self.model or not self.minimums_data:
            return
        
        print("\\nüß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ù–ê –ü–†–ò–ú–ï–†–ê–•")
        print("=" * 35)
        
        # –ë–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤
        examples = self.minimums_data[:5]
        
        for i, example in enumerate(examples):
            print(f"\\nüìä –ü—Ä–∏–º–µ—Ä {i+1}: {example['symbol']} {example['time'][:10]}")
            print(f"   –†–µ–∞–ª—å–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç: {'‚úÖ –ü—Ä–∏–±—ã–ª—å–Ω—ã–π' if example['is_profitable'] else '‚ùå –£–±—ã—Ç–æ—á–Ω—ã–π'}")
            print(f"   –ü—Ä–∏–±—ã–ª—å —á–µ—Ä–µ–∑ 24—á: {example['future_profit_24h']:.2f}%")
            
            # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
            prediction = self.calculate_minimum_probability(example['criteria'])
            
            if 'error' not in prediction:
                print(f"   ü§ñ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏: {'‚úÖ –ú–∏–Ω–∏–º—É–º' if prediction['is_minimum'] else '‚ùå –ù–µ –º–∏–Ω–∏–º—É–º'}")
                print(f"   üìä –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –º–∏–Ω–∏–º—É–º–∞: {prediction['probability']:.1%}")
                print(f"   ‚öñÔ∏è –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞: {prediction['weighted_score']:.3f}")
            else:
                print(f"   ‚ùå {prediction['error']}")

if __name__ == "__main__":
    trainer = WeightedMLTrainer()
    
    print("üß† –í–ó–í–ï–®–ï–ù–ù–´–ô ML –¢–†–ï–ù–ï–† –î–õ–Ø –ú–ò–ù–ò–ú–£–ú–û–í")
    print("‚öñÔ∏è –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –∫—Ä–∏—Ç–µ—Ä–∏—è")
    print("=" * 45)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ
    if trainer.load_minimums_data("minimums_202501.json"):
        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤–∞–∂–Ω–æ—Å—Ç—å –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
        trainer.analyze_feature_importance()
        
        # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
        if trainer.train_model():
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
            trainer.save_model()
            
            # –¢–µ—Å—Ç–∏—Ä—É–µ–º –Ω–∞ –ø—Ä–∏–º–µ—Ä–∞—Ö
            trainer.test_model_on_examples()
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—É—á–∏—Ç—å –º–æ–¥–µ–ª—å!")
    else:
        print("‚ùå –°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ simple_min_detector.py –¥–ª—è —Å–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö!")


